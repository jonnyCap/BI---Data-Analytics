{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda create -n BI2025 python=3.11 -y\n",
    "#!conda activate BI2025\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5122654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79408d3",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a95c",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_by ='stud-id_12205610'  # Replace the digits after \"id_\" with your own student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2160a7",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16721334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group id for this project\n",
    "group_id = '84'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12205610'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_12129099'  # Replace the digits after \"id_\" with student B's student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb927186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e253f6",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cee91",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    'unit': 'http://qudt.org/vocab/unit/', # Added by student_a\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970468d",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62393d",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  +\"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a1605",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "reigstration_triples_a = [\n",
    "f':{student_a} rdf:type foaf:Person .',\n",
    "f':{student_a} rdf:type prov:Agent .',\n",
    "f':{student_a} foaf:givenName \"Jonathan\" .',\n",
    "f':{student_a} foaf:familyName \"Maier\" .',\n",
    "f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"12205610\" .',\n",
    "f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"01234567\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "f':{student_b} rdf:type foaf:Person .',\n",
    "f':{student_b} rdf:type prov:Agent .',\n",
    "f':{student_b} foaf:givenName \"Morgan\" .',\n",
    "f':{student_b} foaf:familyName \"Stern\" .',\n",
    "f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"76543210\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479ed4",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee069d",
   "metadata": {},
   "source": [
    "## Business Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee88389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':business_understanding_phase rdf:type prov:Activity .',\n",
    "f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .', ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc8a3a-708a-4992-a076-038c53338e89",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd9643d1e26a8dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "Scenario:       We founded the Company \"Fritz & Maier AG\" after our Bachelors, which is a software company that has scaled rapidly to hundreds of employees that are located all over the world. \n",
    "                Due to this rapid expansion, the current compensation structure is ad-hoc, resulting in internal pay inequities and potential misalignment with the current market. \n",
    "                As the company matures and growes even further, HR requires a data-driven approach to standardize these salaries. \n",
    "                The goal is to use the Stack Overflow data to analyze the relationship between skills, experience, and education against compensation to benchmark current employee salaries make adaptions for the following business year.\n",
    "\n",
    "Data Source:    The analysis utilizes the Stack Overflow Annual Developer Survey 2025. \n",
    "                This dataset contains comprehensive information regarding developer demographics, experience, technology usage, and compensation from a global audience.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "The primary objective is to establish a data-driven salary benchmarking model to find a fair market value for software engineering roles based on objective factors such as experience, location and their tech stack.\n",
    "\n",
    "Secondary Objectives:\n",
    "1. Identify current employees who may be significantly under- or overpaid compared to the market.\n",
    "2. Standardize salary offers for new hires to reduce negotiation time.\n",
    "3. Ensure the company's budget is optimized by avoiding arbitrary overpayment while maintaining competitiveness.\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "1. Salary Model Adoption: The HR department successfully adopts the model to review at least 80% of current employee contracts.\n",
    "2. Turnover Reduction: A measurable reduction in staff turnover attributed to compensation issues.\n",
    "3. Offer Acceptance: An increase in the acceptance rate of initial job offers for new hires, indicating that the data-backed offers are competitive.\n",
    "5. Budget Optimization: Achieving a total salary budget that aligns within +/- 10% of the predicted market benchmark, ensuring the company is neither overspending nor significantly underpaying.\n",
    "6. Faster Time-to-Hire: A reduction in the average time required to close a candidate, specifically by shortening the salary negotiation phase through transparent, data-backed offers.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "The general overall goal of our data mining activites is to build a regression model which helps HR determine appropriate salaries for new employees. \n",
    "And in order to to that we take the for us most important pieces of data out of the Developer survey, divide it into different pieces of data categories and then start to transform the data.\n",
    "Our second data mining goal is to identify employees that are significantly over- / underpaid in comparison to the market.\n",
    "And our third goal is to identify which skills or attributes influences the salary of a developer the most. For this we do not only take the most prominent ones into account like the years of working experience,\n",
    "but also attributes that seem not interesting at the first glance, for example the industry a developer is working in.\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "### Source of how R^2 works: https://statisticsbyjim.com/regression/interpret-r-squared-regression/ ###\n",
    "The first and most important data mining success criteria is to achieve a high accuracy regarding the salary predictions. Therefore we want to reach an R^2 of 80% minimum.\n",
    "Our second goal is to have as less outliers as possible, in order to consider our model as robust. We consider an employee to be an outlier, if he has +/- 20% deviation of the initial model prediction.\n",
    "Our third goal is to have a high data quality for our model, which means, that after all data cleansing we have less than 10% of missing data.\n",
    "Our fourth goal is to make the model readable for the HR department. As this is a subjective criteria it has to be checked, whether the employees of the HR department are\n",
    "able to properly read the model and take conclusions out of it. In order to guarantee that, the creators of the model validate the usage of the HR department.\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "Fairness & Ethics:\n",
    "    -) The big advantage the chosen dataset has is that no differentiation between male and female developers has been made which means that the risk of the model building a bias in that regard is non existent.\n",
    "    -) We also want to mitigate the risk of noisy or missing data as much as possible in order to also not have the potential problem of unreliability within the model.\n",
    "    -) In our opinion the only possible problem / bias the model could have is the country that a developer is working in. \n",
    "Data quality:\n",
    "    -) Even though, the stack overflow survey consists of a broad range of developers that were asked, it does not represent the global deveveloper population or their salaries.\n",
    "    -) As the Fritz & Maier AG currently has their very own unique way of setting salaries, it may be possible, that the trained model strongly misaligns with the current salaries and therefore could be useless.\n",
    "Documentation & Usage:\n",
    "    -) Especially in fields like the salaries it is very important to have a high governance and documentation, in order to not only be compliant with the EU AI-Act, but also for the HR department to know, that the\n",
    "       model can be used without having to think about biases at all.\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "bu_ass_uuid_executor = \"bb6a40f9-9d92-4f9f-bbd2-b65ef6a82da2\" # Generate once\n",
    "business_understanding_executor = [\n",
    "f':business_understanding rdf:type prov:Activity .',\n",
    "f':business_understanding sc:isPartOf :business_understanding_phase .', # Connect Activity to Parent Business Understanding Phase Activity\n",
    "f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "# 1a\n",
    "f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "# 1b\n",
    "f':bu_business_objectives rdf:type prov:Entity .',\n",
    "f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "# 1c\n",
    "f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "# 1d\n",
    "f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "# 1e\n",
    "f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "# 1f\n",
    "f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9b28",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "    f':data_understanding_phase rdf:type prov:Activity .',\n",
    "    f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .', \n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce717fb",
   "metadata": {},
   "source": [
    "#### Data Schema and Layout\n",
    "Before we dive deeper into the identified issues, the following section provides a brief overview of the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "survey_data_path = os.path.join(\"data\", \"stack-overflow-developer-survey-2025\")\n",
    "load_survey_data_code_writer = student_a\n",
    "def load_survey_data()-> pd.DataFrame:\n",
    "    ### Load your data\n",
    "    input_file = os.path.join(survey_data_path, 'survey_results_public.csv')\n",
    "    return pd.read_csv(input_file,  sep=',', header = 0, low_memory=False)\n",
    "\n",
    "def load_survey_schema() -> pd.DataFrame:\n",
    "    ### Load your data schema\n",
    "    schema_file = os.path.join(survey_data_path, 'survey_results_schema.csv')\n",
    "    return pd.read_csv(schema_file, sep=',', header=0, low_memory=False)\n",
    "    \n",
    "start_time_ld = now()\n",
    "data = load_survey_data()\n",
    "schema = load_survey_schema()\n",
    "end_time_ld = now()\n",
    "\n",
    "# show all columns for this display only\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(\"Schema:\")\n",
    "    display(schema.head())\n",
    "    print(\"\\n\\nData:\")\n",
    "    display(data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation \n",
    "# AI DISCLAIMER: I created a few documentations on my own assisted by Gemini.\n",
    "# After a few exmples were created I used Gemini to generate the rest, while I was just then just adapting and correcting them.\n",
    "# #############################################\n",
    "\n",
    "\n",
    "# Always add these triples for every activity to define the executor!\n",
    "ld_ass_uuid_executor = \"2fd35d2e-784c-4f18-be0f-cd78bba48813\" # Generate once\n",
    "load_survey_data_executor = [\n",
    "    f':load_survey_data prov:qualifiedAssociation :{ld_ass_uuid_executor} .',\n",
    "    f':{ld_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ld_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "\n",
    "ld_ass_uuid_writer = \"68894ca0-0692-4e99-a19d-48b5263d941b\" # Fixed UUID\n",
    "ld_report = \"Load Stack Overflow 2025 Survey Data and Schema CSVs into Pandas DataFrames.\"\n",
    "load_survey_activity = [\n",
    "    # Activity Definition\n",
    "    ':load_survey_data rdf:type prov:Activity .',\n",
    "    ':load_survey_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_survey_data rdfs:label \"Load Survey Data\" .',\n",
    "    f':load_survey_data rdfs:comment \"\"\"{ld_report}\"\"\" .',\n",
    "    f':load_survey_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_survey_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "    \n",
    "    # Agent Association (Writer)\n",
    "    f':load_survey_data prov:qualifiedAssociation :{ld_ass_uuid_writer} .',\n",
    "    f':{ld_ass_uuid_writer} prov:agent :{load_survey_data_code_writer} .',\n",
    "    f':{ld_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # Inputs (The Files)\n",
    "    ':load_survey_data prov:used :survey_csv_file .',\n",
    "    ':load_survey_data prov:used :schema_csv_file .',\n",
    "    \n",
    "    # Outputs (The Loaded Data Entity)\n",
    "    ':loaded_survey_data rdf:type prov:Entity .',\n",
    "    ':loaded_survey_data prov:wasGeneratedBy :load_survey_data .',\n",
    "    ':loaded_survey_data prov:wasDerivedFrom :survey_csv_file .',\n",
    "]\n",
    "engine.insert(load_survey_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Define the File Assets (Croissant)\n",
    "file_asset_triples = [\n",
    "    # Main Data File\n",
    "    ':survey_csv_file rdf:type cr:FileObject .',\n",
    "    ':survey_csv_file sc:name \"survey_results_public.csv\" .',\n",
    "    ':survey_csv_file sc:description \"Raw survey responses from Stack Overflow 2025\" .',\n",
    "    ':survey_csv_file sc:encodingFormat \"text/csv\" .',\n",
    "    \n",
    "    # Schema File\n",
    "    ':schema_csv_file rdf:type cr:FileObject .',\n",
    "    ':schema_csv_file sc:name \"survey_results_schema.csv\" .',\n",
    "    ':schema_csv_file sc:description \"Metadata and question definitions\" .',\n",
    "    ':schema_csv_file sc:encodingFormat \"text/csv\" .',\n",
    "]\n",
    "engine.insert(file_asset_triples, prefixes=prefixes)\n",
    "\n",
    "# 3. Define the Dataset Schema (Dynamic Generation)\n",
    "dataset_triples = [\n",
    "    ':loaded_survey_data rdf:type sc:Dataset .',\n",
    "    ':loaded_survey_data sc:name \"Stack Overflow Developer Survey 2025\" .',\n",
    "    ':loaded_survey_data sc:description \"Public results of the annual developer survey.\" .',\n",
    "    ':loaded_survey_data sc:distribution :survey_csv_file .',\n",
    "    \n",
    "    # Define the RecordSet\n",
    "    ':survey_recordset rdf:type cr:RecordSet .',\n",
    "    ':survey_recordset sc:name \"Survey Responses\" .',\n",
    "    ':loaded_survey_data cr:recordSet :survey_recordset .',\n",
    "]\n",
    "\n",
    "# --- AUTOMATED FIELD GENERATION ---\n",
    "for col_name in data.columns:\n",
    "    # We replace anything that is NOT a letter, number, or underscore with an underscore\n",
    "    safe_col_id = re.sub(r'[^a-zA-Z0-9_]', '_', col_name)\n",
    "    field_node = f\":field_{safe_col_id}\"\n",
    "    \n",
    "    # Get description from Schema DF if available\n",
    "    try:\n",
    "        desc = \"No description available\"\n",
    "        if col_name in schema['qname'].values:\n",
    "            desc = schema.loc[schema['qname'] == col_name, 'question'].values[0]\n",
    "        else:\n",
    "            # Fallback for matrix sub-columns\n",
    "            for q in sorted(schema['qname'].astype(str), key=len, reverse=True):\n",
    "                if col_name.startswith(q):\n",
    "                    desc = schema.loc[schema['qname'] == q, 'question'].values[0]\n",
    "                    break\n",
    "        \n",
    "        # Clean text for RDF (Escape quotes and remove newlines)\n",
    "        desc = desc.replace('\"', \"'\").replace('\\n', ' ')[:200]\n",
    "    except:\n",
    "        desc = \"Description lookup failed\"\n",
    "\n",
    "    # Infer XSD Data Type from Pandas Dtype\n",
    "    dtype = data[col_name].dtype\n",
    "    xsd_type = \"xsd:string\"\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        xsd_type = \"xsd:integer\"\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        xsd_type = \"xsd:double\"\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        xsd_type = \"xsd:boolean\"\n",
    "\n",
    "    # Add Field Triples\n",
    "    dataset_triples.append(f':survey_recordset cr:field {field_node} .')\n",
    "    dataset_triples.append(f'{field_node} rdf:type cr:Field .')\n",
    "    dataset_triples.append(f'{field_node} sc:name \"{col_name}\" .') # Keep original name here\n",
    "    dataset_triples.append(f'{field_node} sc:description \"{desc}\" .')\n",
    "    dataset_triples.append(f'{field_node} cr:dataType {xsd_type} .')\n",
    "\n",
    "    # Add QUDT Units for specific columns\n",
    "    if col_name == 'ConvertedCompYearly':\n",
    "         dataset_triples.append(f'{field_node} qudt:unit unit:US_Dollar .')\n",
    "    elif col_name in ['WorkExp', 'YearsCode']:\n",
    "         dataset_triples.append(f'{field_node} qudt:unit unit:Year .')\n",
    "\n",
    "# Insert the massive list of triples\n",
    "engine.insert(dataset_triples, prefixes=prefixes)\n",
    "print(\"Documentation complete: Survey Data loaded and schema defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f310e",
   "metadata": {},
   "source": [
    "### Understanding the Question Schemas\n",
    "\n",
    "To facilitate a systematic understanding of the dataset structure, we utilize the auxiliary `survey_results_schema.csv` file. This metadata file provides essential context for each column in the primary dataset, specifically defining the 'type' of question associated with each feature. By mapping these types to our data columns, we can categorize features into logical groups for appropriate analysis and preprocessing.\n",
    "\n",
    "The schema identifies four primary question types:\n",
    "\n",
    "1.  **Rank Order (RO):** Questions where respondents rank items by preference or importance (e.g., '1' = Most Important).\n",
    "2.  **Matrix:** Questions allowing multiple selections from a list, often resulting in semicolon-separated string values.\n",
    "3.  **Text Entry (TE):** Open-ended fields that may contain either unstructured free text or numeric input that requires cleaning.\n",
    "4.  **Multiple Choice (MC):** Categorical questions where respondents select a single option from a predefined list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c62e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the schema by 'type' and print the column names for each\n",
    "for q_type, group in schema.groupby('type'):\n",
    "    cols = group['qname'].unique().tolist()\n",
    "    print(f\"\\n### Question Type: {q_type} ({len(cols)} columns) ###\")\n",
    "    print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ed129",
   "metadata": {},
   "source": [
    "## Identifying Schema Mismatch\n",
    "Unfortunately we relied on the correctness of the schema but then discovered late in the Data Understanding phase that its column list does not fully match the actual data's CSV header. So this misalignment has to be kept in mind whenever we reference the schema in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e57394",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_columns = schema['qname'].tolist()\n",
    "schema_column_set = set(schema_columns)\n",
    "data_column_set = set(data.columns)\n",
    "\n",
    "common_count = len(schema_column_set & data_column_set)\n",
    "data_only_count = len(data_column_set - schema_column_set)\n",
    "schema_only = [col for col in schema_columns if col not in data_column_set]\n",
    "\n",
    "print(f\"Number of columns in both schema and data: {common_count}\")\n",
    "print(f\"Number of columns only in data: {data_only_count}\")\n",
    "print(f\"Number of columns only in schema: {len(schema_only)}\")\n",
    "\n",
    "# --- Documentation ---\n",
    "validation_summary = f\"\"\"\n",
    "Schema Consistency Analysis:\n",
    "- Common Columns: {common_count}\n",
    "- Columns only in Data: {data_only_count}\n",
    "- Columns only in Schema: {len(schema_only)}\n",
    "\"\"\"\n",
    "\n",
    "# Generated uuid\n",
    "sc_check_uuid = \"a1b2c3d4-e5f6-7890-1234-56789abcdef0\" \n",
    "schema_check_triples = [\n",
    "    # --- The Activity ---\n",
    "    ':check_schema_consistency rdf:type prov:Activity .',\n",
    "    ':check_schema_consistency rdfs:label \"Check Schema vs Data Consistency\" .',\n",
    "    ':check_schema_consistency sc:isPartOf :data_understanding_phase .',\n",
    "    f':check_schema_consistency prov:startedAtTime \"{now()}\"^^xsd:dateTime .',\n",
    "    \n",
    "    # Inputs\n",
    "    ':check_schema_consistency prov:used :loaded_survey_data .',\n",
    "    ':check_schema_consistency prov:used :schema_csv_file .',\n",
    "\n",
    "    # --- The Agent Association (Who did it?) ---\n",
    "    f':check_schema_consistency prov:qualifiedAssociation :{sc_check_uuid} .',\n",
    "    f':{sc_check_uuid} rdf:type prov:Association .',\n",
    "    f':{sc_check_uuid} prov:agent :{executed_by} .',\n",
    "    f':{sc_check_uuid} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # --- The Output (The Report Entity) ---\n",
    "    ':schema_mismatch_report rdf:type prov:Entity .',\n",
    "    ':schema_mismatch_report rdfs:label \"Schema Consistency Report\" .',\n",
    "    f':schema_mismatch_report rdfs:comment \"\"\"{validation_summary}\"\"\" .',\n",
    "    ':schema_mismatch_report prov:wasGeneratedBy :check_schema_consistency .',\n",
    "]\n",
    "\n",
    "# 4. Insert into Graph\n",
    "engine.insert(schema_check_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70f296",
   "metadata": {},
   "source": [
    "### Temporary Preprocessing for Data Understanding\n",
    "\n",
    "So far we have found out what types of information the columns store, but to do a more detailed analysis we have to do a short preprocessing, simplifying the foundation for the following steps. So in the following we will already apply some steps that would belong to the Data Preparation Part, but to narrow the focus of the Data Understanding, allowing more detail, these will already be applied here.\n",
    "\n",
    "First of all, since the dataset feeds a regression model targeting the `ConvertedCompYearly` (salary) column, we drop every record missing that value. Then we also omit the open-ended questions to avoid manual or LLM-assisted analysis. There was an exception made for the columns: `YearsCode`, `CompTotal` and `WorkExp`, as these are clear numeric columns that can be converted.\n",
    "\n",
    "Additionally multiple columns were dropped, as they dont seem relevant for our business case or contain subjective noise/meta-data that does not correlate strongly with salary. Due to the mismatch between the columns in the schema and in the actual dataset, we first went through the schmema to select the columns to drop, but then had to additionally go through the dataset to select the columns there as well. This is why there are potentially column names listed to be dropped, while not even existent in the csv.\n",
    "\n",
    "#### Ranking Columns Disclaimer\n",
    "The survey also included some ranking questions, which now have 15 columns each in the final dataset. The next section will handle those specifically, for now they are left in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a60c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_from_schema = [\n",
    "    # --- AI & Learning (Noise/Opinion) ---\n",
    "    'AIAgentChallenges', 'AIAgentChange', 'AIAgentImpact', 'AIAgent_Uses', \n",
    "    'AIAgents', 'AIExplain', 'AIFrustration', 'AILearnHow', \n",
    "    'AIModels', 'AIModelsChoice', 'AIThreat', 'AITool', \n",
    "    'AgentUsesGeneral', 'LearnCode', 'LearnCodeAI', 'LearnCodeChoose', \n",
    "\n",
    "    # --- Survey Metadata & S.O. Activity (Noise) ---\n",
    "    'ICorPM', 'NewRole', 'PurchaseInfluence', 'ToolCountPersonal', 'ToolCountWork',\n",
    "    'CommPlatform', 'SOAccount', 'SOComm', 'SODuration', 'SOFriction', \n",
    "    'SOPartFreq', 'SOTags', 'SOVisitFreq', 'SO_Dev_Content',\n",
    "]\n",
    "\n",
    "cols_to_drop_final = [\n",
    "    # Leakage & ID\n",
    "    'CompTotal', 'ResponseId', 'Currency',\n",
    "    \n",
    "    # Future/Sentiment (Want/Admired/Choice)\n",
    "    'LanguageWantToWorkWith', 'LanguageAdmired', 'LanguageChoice',\n",
    "    'DatabaseWantToWorkWith', 'DatabaseAdmired', 'DatabaseChoice',\n",
    "    'PlatformWantToWorkWith', 'PlatformAdmired', 'PlatformChoice',\n",
    "    'WebframeWantToWorkWith', 'WebframeAdmired', 'WebframeChoice',\n",
    "    'DevEnvsWantToWorkWith', 'DevEnvsAdmired', 'DevEnvsChoice',\n",
    "    'SOTagsWantToWorkWith', 'SOTagsAdmired', 'SOTagsHaveWorkedWith',\n",
    "    'OfficeStackAsyncWantToWorkWith', 'OfficeStackAsyncAdmired', 'OfficeStackAsyncHaveWorkedWith',\n",
    "    'CommPlatformWantToWorkWith', 'CommPlatformAdmired', 'CommPlatformHaveWorkedWith',\n",
    "    'AIModelsWantToWorkWith', 'AIModelsAdmired', 'AIModelsHaveWorkedWith',\n",
    "    \n",
    "    # Personal & AI Noise\n",
    "    'OpSysPersonal use', \n",
    "    'AIAcc', 'AIComplex', 'AIHuman','TechEndorseIntro',\n",
    "    \n",
    "    # Granular AI Tool/Agent columns\n",
    "    'AIToolCurrently partially AI', \"AIToolDon't plan to use AI for this task\", \n",
    "    'AIToolPlan to partially use AI', 'AIToolPlan to mostly use AI', 'AIToolCurrently mostly AI',\n",
    "    'AIAgentImpactSomewhat agree', 'AIAgentImpactNeutral', 'AIAgentImpactSomewhat disagree', \n",
    "    'AIAgentImpactStrongly agree', 'AIAgentImpactStrongly disagree',\n",
    "    'AIAgentChallengesNeutral', 'AIAgentChallengesSomewhat disagree', 'AIAgentChallengesStrongly agree', \n",
    "    'AIAgentChallengesSomewhat agree', 'AIAgentChallengesStrongly disagree',\n",
    "    'AIAgentKnowledge', 'AIAgentOrchestration', 'AIAgentObserveSecure', 'AIAgentExternal'\n",
    "]\n",
    "\n",
    "cols_to_drop = cols_to_drop_from_schema + cols_to_drop_final\n",
    "\n",
    "# Create a dictionary to map column names to their question type from the schema\n",
    "# This handles the fact that some data columns (like 'LanguageHaveWorkedWith') map to a 'qname' in schema (like 'Language')\n",
    "schema_map = dict(zip(schema['qname'], schema['type']))\n",
    "sorted_qnames = sorted(schema['qname'].astype(str).tolist(), key=len, reverse=True)\n",
    "\n",
    "def get_col_type(col_name):\n",
    "    if col_name in schema_map: return schema_map[col_name]\n",
    "    for q in sorted_qnames:\n",
    "        if col_name.startswith(q): return schema_map[q]\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Apply type mapping\n",
    "column_types = {col: get_col_type(col) for col in data.columns}\n",
    "\n",
    "print(\"Data Loaded. Total Columns:\", len(data.columns))\n",
    "print(\"Example Type Mapping:\", list(column_types.items())[:5])\n",
    "\n",
    "keep_cols = []\n",
    "drop_cols = []\n",
    "\n",
    "# Numeric concepts hidden in Text Entry (TE) fields that we WANT\n",
    "NUMERIC_KEYWORDS = ['YearsCode', 'WorkExp']\n",
    "\n",
    "for col, q_type in column_types.items():\n",
    "    if col == 'ConvertedCompYearly':\n",
    "        keep_cols.append(col)\n",
    "        continue\n",
    "    \n",
    "    # Drop explicit text inputs\n",
    "    if '_TEXT' in col:\n",
    "        drop_cols.append(col)\n",
    "        continue\n",
    "\n",
    "    if col in cols_to_drop:\n",
    "        drop_cols.append(col)\n",
    "        continue\n",
    "        \n",
    "    # Logic for Text Entry (TE)\n",
    "    if q_type == 'TE':\n",
    "        if any(k in col for k in NUMERIC_KEYWORDS):\n",
    "            print(f\"Keeping TE column as numeric: {col}\")\n",
    "            keep_cols.append(col) # It's likely numeric (e.g. YearsCode)\n",
    "        else:\n",
    "            drop_cols.append(col) # It's likely just comments/text\n",
    "    else:\n",
    "        keep_cols.append(col)\n",
    "\n",
    "# Create the filtered dataframe for analysis\n",
    "df_analysis = data[keep_cols].copy()\n",
    "\n",
    "print(f\"Dropped {len(drop_cols)} text columns.\")\n",
    "print(f\"Retained {len(keep_cols)} columns for analysis.\")\n",
    "\n",
    "# Drop Rows explicitly\n",
    "target_col = 'ConvertedCompYearly'\n",
    "nan_target_mask = df_analysis[target_col].isna()\n",
    "rows_dropped = int(nan_target_mask.sum())\n",
    "rows_remaining = int((~nan_target_mask).sum())\n",
    "print(f\"Rows dropped due to NaN in {target_col}: {rows_dropped}\")\n",
    "print(f\"Rows remaining after drop: {rows_remaining}\")\n",
    "salary_data = df_analysis.loc[~nan_target_mask, target_col]\n",
    "\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_analysis.head(0))\n",
    "    print(salary_data.head(10))\n",
    "\n",
    "\n",
    "# --- Documentation of Column Selection Step ---\n",
    "drop_columns_report = f\"\"\"\n",
    "Columns Dropped for Analysis ({len(drop_cols)} total):\n",
    "- {', '.join(drop_cols)}\n",
    "Columns Retained for Analysis ({len(keep_cols)} total):\n",
    "- {', '.join(keep_cols)}\n",
    "\"\"\"\n",
    "\n",
    "# --- Documentation ---\n",
    "column_selection_uuid = \"d4e5f6a7-b4c9-0127-4567-89abcdof0123\"\n",
    "column_selection_activity = \"column_selection\"\n",
    "column_selection = [\n",
    "    # --- The Activity ---\n",
    "    f':{column_selection_activity} rdf:type prov:Activity .',\n",
    "    f':{column_selection_activity} rdfs:label \"Check Schema vs Data Consistency\" .',\n",
    "    f':{column_selection_activity} sc:isPartOf :data_understanding_phase .',\n",
    "    f':{column_selection_activity} prov:startedAtTime \"{now()}\"^^xsd:dateTime .',\n",
    "    f':{column_selection_activity} prov:wasAssociatedWith :{student_a} .',\n",
    "    \n",
    "    # Inputs\n",
    "    f':{column_selection_activity} prov:used :loaded_survey_data .',\n",
    "    f':{column_selection_activity} prov:used :schema_csv_file .',\n",
    "\n",
    "    # --- The Agent Association (Who did it?) ---\n",
    "    f':{column_selection_activity} prov:qualifiedAssociation :{column_selection_uuid} .',\n",
    "    f':{column_selection_uuid} rdf:type prov:Association .',\n",
    "    f':{column_selection_uuid} prov:agent :{executed_by} .',\n",
    "    f':{column_selection_uuid} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # --- The Output (The Report Entity) ---\n",
    "    f':{column_selection_activity}_report rdf:type prov:Entity .',\n",
    "    f':{column_selection_activity}_report rdfs:label \"Schema Consistency Report\" .',\n",
    "    f':{column_selection_activity}_report rdfs:comment \"\"\"{drop_columns_report}\"\"\" .',\n",
    "    f':{column_selection_activity}_report prov:wasGeneratedBy :{column_selection_activity} .',\n",
    "]\n",
    "\n",
    "# 4. Insert into Graph\n",
    "engine.insert(column_selection, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c674c",
   "metadata": {},
   "source": [
    "#### Filtering Ranking Columns\n",
    "As there are 15 columns for each Ranking question, this is a lot of noise and might put too much emphasize on the ranking while actually just being a small part of the survey. Therefore in this section we will find the 3 categories for each ranking question that have the highest correlation with the target column. For this we use \"pearson\" method, which calcualtes linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a648a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'ConvertedCompYearly'\n",
    "groups = [\"TechEndorse\", \"TechOppose\", \"JobSatPoints\"]\n",
    "\n",
    "df_analysis[target_col] = pd.to_numeric(df_analysis[target_col], errors='coerce')\n",
    "df_clean = df_analysis.dropna(subset=[target_col]) # safety first\n",
    "\n",
    "\"\"\"try:\n",
    "    df_clean[target_col].corr(df_clean[target_col], method='spearman')\n",
    "    corr_method = 'spearman'\n",
    "    print(\"Using Spearman correlation (statistically better for rankings).\")\n",
    "except Exception:\n",
    "    corr_method = 'pearson'\n",
    "    print(\"Scipy not found. Using Pearson correlation (standard linear correlation).\")\"\"\"\n",
    "\n",
    "corr_method = 'pearson'\n",
    "\n",
    "# Find top 3 each\n",
    "final_selected_columns = []\n",
    "for group in groups:\n",
    "    print(f\"\\n--- Analyzing Group: {group} ---\")\n",
    "    \n",
    "    # regex: starts with group name, ends with underscore and number (example 'TechEndorse_1')\n",
    "    regex_pattern = f\"^{group}_\\d+$\"\n",
    "    group_cols = [c for c in df_clean.columns if re.match(regex_pattern, c)]\n",
    "    \n",
    "    if not group_cols:\n",
    "        print(f\"Warning: No columns found for group '{group}'\")\n",
    "        continue\n",
    "        \n",
    "    # Calculate correlation\n",
    "    correlations = df_clean[group_cols].corrwith(df_clean[target_col], method=corr_method)\n",
    "    top_3 = correlations.abs().sort_values(ascending=False).head(3)\n",
    "    \n",
    "    for col, score in top_3.items():\n",
    "        actual_corr = correlations[col]\n",
    "        print(f\"  {col}: {actual_corr:.4f} (Abs: {score:.4f})\")\n",
    "        final_selected_columns.append(col)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"FINAL COLUMNS TO KEEP\")\n",
    "print(final_selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb314dc",
   "metadata": {},
   "source": [
    "#### Removing Ranking Columns that did not pass the top 3 Correlation Check\n",
    "Now we can remove all the Ranking columsn for TechEndorse, TechOppose and JabSatPoints that are not within the top 3. As the SO_Actions Ranking is entirely irrelevant we drop all columns for this Ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_groups = [\"TechEndorse\", \"TechOppose\", \"JobSatPoints\", \"SO_Actions\"]\n",
    "all_ranking_cols = []\n",
    "\n",
    "for group in ranking_groups:\n",
    "    # Matches 'TechEndorse_1', 'JobSatPoints_12', etc.\n",
    "    regex_pattern = f\"^{group}_\\d+$\"\n",
    "    found = [c for c in df_analysis.columns if re.match(regex_pattern, c)]\n",
    "    all_ranking_cols.extend(found)\n",
    "\n",
    "columns_to_drop = list(set(all_ranking_cols) - set(final_selected_columns))\n",
    "\n",
    "print(f\"Total ranking columns found: {len(all_ranking_cols)}\")\n",
    "print(f\"Keeping: {len(final_selected_columns)}\")\n",
    "print(f\"Dropping: {len(columns_to_drop)}\")\n",
    "\n",
    "# 3. Drop the unused columns from the dataframe\n",
    "df_reduced = df_analysis.drop(columns=columns_to_drop)\n",
    "\n",
    "# Verify the result\n",
    "print(\"\\n--- Columns successfully dropped. ---\")\n",
    "print(f\"Original shape: {df_analysis.shape}\")\n",
    "print(f\"New shape:      {df_reduced.shape}\")\n",
    "\n",
    "# Optional: Check that your target and important non-ranking columns are still there\n",
    "print(f\"\\nVerifying 'ConvertedCompYearly' is present: {'ConvertedCompYearly' in df_reduced.columns}\")\n",
    "print(f\"Verifying 'YearsCode' is present: {'YearsCode' in df_reduced.columns}\")\n",
    "\n",
    "print(\"\\n Final remaining number of columns:\", len(df_reduced.columns))\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_reduced.head(0))\n",
    "\n",
    "\n",
    "# Set df_analysis for furtherer steps\n",
    "df_analysis = df_reduced.copy()\n",
    "\n",
    "# --- Documentation ---\n",
    "ranking_report_text = f\"\"\"\n",
    "Correlation Method: {corr_method}\n",
    "Selected Top-3 Ranking Features:\n",
    "{', '.join(final_selected_columns)}\n",
    "Dropped Ranking Features:\n",
    "{', '.join(columns_to_drop)}\n",
    "\"\"\"\n",
    "\n",
    "ranking_uuid = \"f8a9b0c1-d2e3-4567-8901-23456abcdef7\" \n",
    "ranking_activity = \"select_ranking_features\"\n",
    "ranking_selection_triples = [\n",
    "    f':{ranking_activity} rdf:type prov:Activity .',\n",
    "    f':{ranking_activity} rdfs:label \"Select Ranking Features via Correlation\" .',\n",
    "    f':{ranking_activity} sc:isPartOf :data_understanding_phase .',\n",
    "    f':{ranking_activity} prov:startedAtTime \"{now()}\"^^xsd:dateTime .',\n",
    "    f':{ranking_activity} prov:used :loaded_survey_data .',\n",
    "    f':{ranking_activity} prov:wasAssociatedWith :{student_a} .',\n",
    "\n",
    "    f':{ranking_activity} prov:qualifiedAssociation :{ranking_uuid} .',\n",
    "    f':{ranking_uuid} rdf:type prov:Association .',\n",
    "    f':{ranking_uuid} prov:agent :{executed_by} .',\n",
    "    f':{ranking_uuid} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    f':ranking_feature_report rdf:type prov:Entity .',\n",
    "    f':ranking_feature_report rdfs:label \"Ranking Feature Selection Report\" .',\n",
    "    f':ranking_feature_report rdfs:comment \"\"\"{ranking_report_text}\"\"\" .',\n",
    "    f':ranking_feature_report prov:wasGeneratedBy :{ranking_activity} .',\n",
    "]\n",
    "\n",
    "engine.insert(ranking_selection_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdcf584",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df653727",
   "metadata": {},
   "source": [
    "## Point A - Semantic Types\n",
    "The following steps involved certain compromises. Semantic types were determined through manual inspection, as we did not find a robust dynamic identifier. For the correlation analysis, only numeric columns were considered, since further analysis would require additional preprocessing. Finally, the plausibility checks focused on key numerical columns, as detailed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6177fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiling_activity_id = \"bb6a40f99d924f9fbdd2b65ef6a82da2\"\n",
    "start_time_profile = now()\n",
    "\n",
    "# --- Point A ---\n",
    "semantic_types = {\n",
    "    \"MainBranch\": \"Categorical\",\n",
    "    \"Age\": \"Ordinal\",\n",
    "    \"EdLevel\": \"Ordinal\",\n",
    "    \"Employment\": \"Categorical\",\n",
    "    \"EmploymentAddl\": \"Multi-Select\",\n",
    "    \"WorkExp\": \"Numerical\",\n",
    "    \"YearsCode\": \"Numerical\",\n",
    "    \"DevType\": \"Categorical\",\n",
    "    \"OrgSize\": \"Ordinal\",\n",
    "    \"RemoteWork\": \"Categorical\",\n",
    "    \"TechEndorse_4\": \"Numerical\",\n",
    "    \"TechEndorse_6\": \"Numerical\",\n",
    "    \"TechEndorse_8\": \"Numerical\",\n",
    "    \"TechOppose_3\": \"Numerical\",\n",
    "    \"TechOppose_9\": \"Numerical\",\n",
    "    \"TechOppose_13\": \"Numerical\",\n",
    "    \"Industry\": \"Categorical\",\n",
    "    \"JobSatPoints_4\": \"Numerical\",\n",
    "    \"JobSatPoints_6\": \"Numerical\",\n",
    "    \"JobSatPoints_11\": \"Numerical\",\n",
    "    \"Country\": \"Categorical\",\n",
    "    \"LanguageHaveWorkedWith\": \"Multi-Select\",\n",
    "    \"DatabaseHaveWorkedWith\": \"Multi-Select\",\n",
    "    \"PlatformHaveWorkedWith\": \"Multi-Select\",\n",
    "    \"WebframeHaveWorkedWith\": \"Multi-Select\",\n",
    "    \"DevEnvsHaveWorkedWith\": \"Multi-Select\",\n",
    "    \"OpSysProfessional use\": \"Multi-Select\",\n",
    "    \"AISelect\": \"Ordinal\",\n",
    "    \"AISent\": \"Ordinal\",\n",
    "    \"AIComplex\": \"Ordinal\",\n",
    "    \"ConvertedCompYearly\": \"Numerical\",\n",
    "    \"JobSat\": \"Numerical\"\n",
    "}\n",
    "\n",
    "attr_triples = []\n",
    "for col in df_analysis.columns:\n",
    "    # Create the triple (Point a)\n",
    "    if(col not in semantic_types):\n",
    "        raise ValueError(f\"Semantic type for column '{col}' not defined.\")\n",
    "    attr_triples.append(f'{field_node} rdfs:comment \"Semantic Type: {semantic_types[col]}\" .')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150cbd1a",
   "metadata": {},
   "source": [
    "## Point B - Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9730438",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict = {}\n",
    "for col in df_analysis.columns:\n",
    "    # Basic metrics\n",
    "    d_type = df_analysis[col].dtype\n",
    "    \n",
    "    # Find more advanced stats based on type\n",
    "    if pd.api.types.is_numeric_dtype(d_type):\n",
    "        skew = df_analysis[col].skew()\n",
    "        kurt = df_analysis[col].kurtosis()\n",
    "        mode_val = df_analysis[col].mode()[0] if not df_analysis[col].mode().empty else \"N/A\"\n",
    "        variance = df_analysis[col].var()\n",
    "        \n",
    "        stats_dict[col] = {\n",
    "            \"mean\": round(df_analysis[col].mean(), 2),\n",
    "            \"std\": round(df_analysis[col].std(), 2),\n",
    "            \"var\": round(variance, 2),\n",
    "            \"min\": df_analysis[col].min(),\n",
    "            \"max\": df_analysis[col].max(),\n",
    "            \"mode\": mode_val,\n",
    "            \"skew\": round(skew, 2),\n",
    "            \"kurtosis\": round(kurt, 2)\n",
    "        }\n",
    "\n",
    "        print(f\"Numeric stats for {col}: {stats_dict[col]}\")\n",
    "    else:\n",
    "        # For categorical columns, describe() is better, but add unique counts\n",
    "        desc = df_analysis[col].describe()\n",
    "        stats_dict[col] = {\n",
    "            \"unique\": int(desc['unique']),\n",
    "            \"top\": desc['top'],\n",
    "            \"freq\": int(desc['freq'])\n",
    "        }\n",
    "\n",
    "        print(f\"Categorical stats for {col}: {stats_dict[col]}\")\n",
    "\n",
    "# Convert to JSON for the Knowledge Graph\n",
    "stats_json = json.dumps(stats_dict, indent=0, default=str).replace('\\n', ' ').replace('\"', \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47b16b",
   "metadata": {},
   "source": [
    "## Point B - Correlation\n",
    "Here we found out if columns heavily correlate with each other or dont propery correlate with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "target_col = 'ConvertedCompYearly' # <--- Your Target\n",
    "collinear_threshold = 0.60         # Threshold for \"significant\" correlation\n",
    "weak_target_threshold = 0.05       # Drop if correlation with target < 0.05\n",
    "# ---------------------\n",
    "\n",
    "# 1. Calculate Matrix\n",
    "df_numeric = df_analysis.select_dtypes(include=[np.number])\n",
    "corr_matrix = df_numeric.corr()\n",
    "\n",
    "# 2. Identify Weak Features\n",
    "target_corrs = corr_matrix[target_col].drop(target_col)\n",
    "weak_features = target_corrs[abs(target_corrs) < weak_target_threshold].index.tolist()\n",
    "\n",
    "# 3. Identify Collinear Features & Prepare JSON\n",
    "collinear_drops = set()\n",
    "strong_corrs = {} # Dictionary for your JSON output\n",
    "\n",
    "columns = corr_matrix.columns\n",
    "for i in range(len(columns)):\n",
    "    for j in range(i):\n",
    "        col_i = columns[i]\n",
    "        col_j = columns[j]\n",
    "        \n",
    "        if col_i == target_col or col_j == target_col: continue\n",
    "            \n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        \n",
    "        # Check threshold\n",
    "        if abs(corr_val) > collinear_threshold:\n",
    "            # A. Prepare data for JSON output\n",
    "            key = f\"{col_i} vs {col_j}\"\n",
    "            strong_corrs[key] = round(corr_val, 2)\n",
    "            \n",
    "            # B. Decide which to drop (Smart Drop)\n",
    "            corr_i_target = abs(corr_matrix.loc[col_i, target_col])\n",
    "            corr_j_target = abs(corr_matrix.loc[col_j, target_col])\n",
    "            \n",
    "            # Drop the one that is LESS predictive of the target\n",
    "            feature_to_drop = col_i if corr_i_target < corr_j_target else col_j\n",
    "            collinear_drops.add(feature_to_drop)\n",
    "\n",
    "# --- OUTPUT ---\n",
    "\n",
    "print(f\"--- 1. WEAK SIGNAL (Corr < {weak_target_threshold} with Target) ---\")\n",
    "print(f\"ACTION: Drop these {len(weak_features)} columns.\")\n",
    "print(weak_features)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"--- 2. MULTICOLLINEARITY (Corr > {collinear_threshold}) ---\")\n",
    "print(f\"ACTION: Drop these {len(collinear_drops)} columns (Keep the better partner).\")\n",
    "print(list(collinear_drops))\n",
    "\n",
    "print(\"\\n--- JSON OUTPUT ---\")\n",
    "# Your requested format:\n",
    "corr_json = json.dumps(strong_corrs, indent=0).replace('\\n', ' ').replace('\"', \"'\")\n",
    "print(corr_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821718ef",
   "metadata": {},
   "source": [
    "## Point C - Plausibility and Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dea457",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report = {}\n",
    "\n",
    "# Missing values\n",
    "missing_counts = df_analysis.isna().sum()\n",
    "quality_report[\"missing_values\"] = missing_counts[missing_counts > 0].to_dict()\n",
    "\n",
    "# Plausibility Checks\n",
    "plausibility_flags = []\n",
    "if 'ConvertedCompYearly' in df_analysis.columns:\n",
    "    salary = df_analysis['ConvertedCompYearly']\n",
    "    if (salary > 1000000).any():\n",
    "        plausibility_flags.append(f\"Target: Found {(salary > 1000000).sum()} salaries > 1M\")\n",
    "    if (salary < 100).any():\n",
    "        plausibility_flags.append(f\"Target: Found {(salary < 100).sum()} salaries < 100\")\n",
    "\n",
    "if 'WorkExp' in df_analysis.columns and pd.api.types.is_numeric_dtype(df_analysis['WorkExp']):\n",
    "    we = df_analysis['WorkExp']\n",
    "    if (we < 0).any():\n",
    "        plausibility_flags.append(\"Implausible: Negative WorkExp found\")\n",
    "    if (we > 60).any():\n",
    "        plausibility_flags.append(f\"Suspicious: {(we > 60).sum()} people with > 60 years WorkExp\")\n",
    "\n",
    "if 'YearsCode' in df_analysis.columns and pd.api.types.is_numeric_dtype(df_analysis['YearsCode']):\n",
    "    yc = df_analysis['YearsCode']\n",
    "    if (yc > 60).any():\n",
    "        plausibility_flags.append(f\"Suspicious: {(yc > 60).sum()} people with > 60 YearsCode\")\n",
    "\n",
    "quality_report[\"plausibility_alerts\"] = plausibility_flags\n",
    "quality_report[\"total_rows\"] = len(df_analysis)\n",
    "quality_json = json.dumps(quality_report, indent=0).replace('\\n', ' ').replace('\"', \"'\")\n",
    "\n",
    "end_time_profile = now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408075f",
   "metadata": {},
   "source": [
    "### Documentation previous Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb40bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiling_query = [\n",
    "    f':{profiling_activity_id} rdf:type prov:Activity .',\n",
    "    f':{profiling_activity_id} rdfs:label \"Automated Data Profiling\" .',\n",
    "    f':{profiling_activity_id} sc:isPartOf :data_understanding_phase .',\n",
    "    f':{profiling_activity_id} prov:startedAtTime \"{start_time_profile}\"^^xsd:dateTime .',\n",
    "    f':{profiling_activity_id} prov:endedAtTime \"{end_time_profile}\"^^xsd:dateTime .',\n",
    "    f':{profiling_activity_id} prov:used :loaded_survey_data .',\n",
    "    f':{profiling_activity_id} prov:wasAssociatedWith :{student_a} .',\n",
    "\n",
    "    # Log the Results (Points b & c)\n",
    "    f':loaded_survey_data rdfs:comment \"\"\"STATS: {stats_json}\"\"\" .',\n",
    "    f':loaded_survey_data rdfs:comment \"\"\"CORRELATIONS: {corr_json}\"\"\" .',\n",
    "    f':loaded_survey_data rdfs:comment \"\"\"CORRELATIONS Disclaimer: Despite the low correlation of the Ranking Columns we still keep them because combined they might provide a useful signal. We will also keep WorkEpxerience for now and see if this provides any issues for the Model.\"\"\" .',\n",
    "    f':loaded_survey_data rdfs:comment \"\"\"QUALITY: {quality_json}\"\"\" .',\n",
    "]\n",
    "# Add attribute semantics (Point a)\n",
    "profiling_query.extend(attr_triples)\n",
    "\n",
    "engine.insert(profiling_query, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93832d1b",
   "metadata": {},
   "source": [
    "## General Data Behaviour/Structure\n",
    "Now that we are familiar with the Data Schema, in the following the overall distributions of the columns is shown. There will be a more detailed examination of the target column `ConvertedCompYearly`. The findings here were especially used for the outlier report, following this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "print(\"--- Generating Individual Distribution Plots ---\")\n",
    "\n",
    "for col in df_analysis.columns:\n",
    "    if df_analysis[col].dropna().empty:\n",
    "        continue\n",
    "        \n",
    "    plt.figure(figsize=(10, 5)) \n",
    "    \n",
    "    # 1. Numeric Data (Histogram + Boxplot)\n",
    "    if pd.api.types.is_numeric_dtype(df_analysis[col]):\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df_analysis[col].dropna(), kde=True, color='skyblue')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=df_analysis[col].dropna(), color='lightgreen')\n",
    "        plt.title(f\"{col} - Boxplot\")\n",
    "        \n",
    "        skew = df_analysis[col].skew()\n",
    "        print(f\"{col}: Numeric | Skew: {skew:.2f}\")\n",
    "\n",
    "    # 2. Categorical / Object Data (Bar Chart)\n",
    "    else:\n",
    "        # Get top 15 categories to prevent to many data points\n",
    "        top_counts = df_analysis[col].value_counts().nlargest(15)\n",
    "        \n",
    "        sns.barplot(x=top_counts.values, y=top_counts.index, palette='viridis', hue=top_counts.index, legend=False)\n",
    "        plt.title(f\"{col} - Top 15 Categories\")\n",
    "        plt.xlabel(\"Count\")\n",
    "        \n",
    "        print(f\"{col}: Categorical | Unique Values: {df_analysis[col].nunique()}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Save the plot to the specified directory\n",
    "    plot_path = os.path.join(\"data\", \"visual_analysis\")\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    plt.savefig(os.path.join(plot_path, f\"{col}_distribution.png\"))\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134b8ad",
   "metadata": {},
   "source": [
    "#### ConvertedCompYear Inspection\n",
    "In this part we will now have a closer look at the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5603a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'ConvertedCompYearly'\n",
    "salary_data = df_analysis.loc[~nan_target_mask, target_col]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 1. Boxplot for Outliers\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(x=salary_data, color='orange')\n",
    "plt.title(\"Salary Boxplot (Detect Outliers)\")\n",
    "plt.xscale('log') # Log scale helps visualize income better\n",
    "\n",
    "# 2. Histogram for Distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(salary_data, kde=True, log_scale=True)\n",
    "plt.title(\"Salary Distribution (Log Scale)\")\n",
    "\n",
    "# 3. Plausibility Check (Scatter vs Experience)\n",
    "df_analysis['WorkExp'] = pd.to_numeric(df_analysis['WorkExp'], errors='coerce')\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(x=df_analysis['WorkExp'], y=salary_data, alpha=0.3)\n",
    "plt.title(\"Salary vs. Work Experience\")\n",
    "plt.ylim(0, 2000000) # Limit y-axis to see the bulk of data, ignoring massive outliers for view\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba78e52",
   "metadata": {},
   "source": [
    "## Visual Exploration Documentation(Point d)\n",
    "So now we have visually explored the data. Below, the most important findings are briefly summarized and discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. LOGGING: Visual Exploration (Point d) ---\n",
    "visual_interpretation = \"\"\"\n",
    "d. Visual Exploration Findings (Comprehensive):\n",
    "   1. Target Variable ('ConvertedCompYearly'): \n",
    "      - Extreme right-skewed distribution. Boxplots confirm massive outliers (> $50M).\n",
    "      - Action: Requires Log transformation and/or IQR outlier clipping (e.g., capping at 99th percentile) before Linear Regression.\n",
    "   \n",
    "   2. Systemic Multi-Label Complexity: \n",
    "      - Features: 'Language', 'Platform', 'Webframe', 'OpSys', 'DevEnvs' are ALL semicolon-separated strings.\n",
    "      - Dominance: 'React', 'Node.js', and 'AWS' are dominant but often appear in combinations (e.g., \"Next.js;Node.js;React\").\n",
    "      - Action: Simple one-hot encoding is insufficient; we must perform string splitting/tokenization to capture individual technologies.\n",
    "\n",
    "   3. Professional Filtering ('MainBranch'):\n",
    "      - The category \"I am a developer by profession\" vastly outweighs others (students, hobbyists).\n",
    "      - Action: To ensure regression quality for salary prediction, we should filter the dataset to include ONLY professional developers.\n",
    "\n",
    "   4. Ordinal & Likert Scale Features:\n",
    "      - 'OrgSize': Categorical but ordinal (ranges from \"Just me\" to \"10,000+ employees\"). Should be encoded numerically to preserve rank.\n",
    "      - Sentiment/Likert ('JobSatPoints', 'TechEndorse', 'TechOppose'): These display discrete, \"spiky\" distributions at integer intervals. While technically ordinal, they can often be treated as continuous features in regression models to capture trends.\n",
    "      - 'JobSat': Left-skewed (peaking at 7-8), indicating a generally satisfied workforce.\n",
    "\n",
    "   5. Industry & Environment:\n",
    "      - 'Industry': Heavily skewed towards \"Software Development\". Other sectors (Fintech, Healthcare) are present but much smaller.\n",
    "      - 'RemoteWork': \"Remote\" and \"Hybrid\" are the norms; \"In-person\" is a distinct minority.\n",
    "      - 'OpSys': High prevalence of \"Windows Subsystem for Linux (WSL)\", indicating that OS usage is no longer binary (Windows vs. Linux) but often hybrid.\n",
    "\n",
    "   6. Experience vs. Salary:\n",
    "      - 'WorkExp' and 'YearsCode' are right-skewed (peaking ~5-10 years) but have long tails.\n",
    "      - Boxplots show realistic outliers (e.g., 40+ years experience), unlike Salary which has unrealistic extremes.\n",
    "\"\"\"\n",
    "\n",
    "vis_activity_id = \"0a82efc5-8a98-41e2-b3ba-33973a0e6186\"\n",
    "\n",
    "vis_query = [\n",
    "    f':{vis_activity_id} rdf:type prov:Activity .',\n",
    "    f':{vis_activity_id} rdfs:label \"Visual Exploration of Data\" .',\n",
    "    f':{vis_activity_id} sc:isPartOf :data_understanding_phase .',\n",
    "    f':{vis_activity_id} prov:startedAtTime \"{now()}\"^^xsd:dateTime .',\n",
    "    f':{vis_activity_id} prov:wasAssociatedWith :{student_a} .',\n",
    "    f':{vis_activity_id} prov:used :loaded_survey_data .',\n",
    "    \n",
    "    f':visual_report_{vis_activity_id} rdf:type prov:Entity .',\n",
    "    f':visual_report_{vis_activity_id} rdfs:label \"Visual Interpretation Report\" .',\n",
    "    f':visual_report_{vis_activity_id} rdfs:comment \"\"\"{visual_interpretation}\"\"\" .',\n",
    "    f':visual_report_{vis_activity_id} prov:wasGeneratedBy :{vis_activity_id} .',\n",
    "]\n",
    "\n",
    "engine.insert(vis_query, prefixes=prefixes)\n",
    "print(\"Visual Exploration Interpretation logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0b70b",
   "metadata": {},
   "source": [
    "### Advanced Outlier Report\n",
    "The visual Exploration of the data allows us to identify the necessary outlier detection actions based on the nature of the data. While most of the columns simply require a Frequence Threshold (kepping top n categories, groupnig remaining entries in \"others\") or to drop some categories, the numerical columns such as `ConvertedCompYearly`, `WorkExp` and `YearsCode`, are the ones that require additional attention to find the outliers.\n",
    "\n",
    "In Additiona to the previous Findings, this Section does an in depth Outlier report. For the outlier detection we use the IQR Method as the Z-Score assumes a Bell-Curve, which does not fit in our case, as the columns checked here are not normally distributed but heavily right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_outliers_code_writer = student_a\n",
    "\n",
    "# Updated list of columns to check\n",
    "cols_to_check = [\"ConvertedCompYearly\", 'WorkExp', 'YearsCode']\n",
    "print(cols_to_check)\n",
    "\n",
    "def check_outliers(data: pd.DataFrame, col: str) -> dict:\n",
    "    tmp = data.copy() # work on copy\n",
    "    \n",
    "    # Coerce to numeric (errors='coerce' turns non-numbers to NaN)\n",
    "    values = pd.to_numeric(tmp[col], errors='coerce')\n",
    "    valid = values.dropna()\n",
    "    \n",
    "    if valid.empty:\n",
    "        return {}\n",
    "\n",
    "    # --- IQR CALCULATION ---\n",
    "    Q1 = valid.quantile(0.25)\n",
    "    Q3 = valid.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers_mask = (values < lower_bound) | (values > upper_bound)\n",
    "    outliers_idx = values[outliers_mask].index\n",
    "    \n",
    "    # Store outlier details\n",
    "    outlier_list = [\n",
    "        {\n",
    "            'index': int(idx),\n",
    "            'value': float(values.loc[idx]),\n",
    "            'reason': 'High' if values.loc[idx] > upper_bound else 'Low'\n",
    "        }\n",
    "        for idx in outliers_idx\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'outliers': outlier_list,\n",
    "        'stats': {\n",
    "            'Q1': float(Q1), \n",
    "            'Q3': float(Q3), \n",
    "            'IQR': float(IQR), \n",
    "            'Upper_Cutoff': float(upper_bound),\n",
    "            'Lower_Cutoff': float(lower_bound)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Check for all specified columns\n",
    "start_time_co = now()\n",
    "outliers_report = {}\n",
    "for col in cols_to_check:\n",
    "    if col not in df_analysis.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "    result = check_outliers(df_analysis, col)\n",
    "    if result:\n",
    "        outliers_report[col] = result['outliers']\n",
    "        outliers_report[f'{col}_stats'] = result['stats']\n",
    "            \n",
    "        # Print for immediate feedback\n",
    "        count = len(result['outliers'])\n",
    "        limit = result['stats']['Upper_Cutoff']\n",
    "        print(f\"[{col}] Found {count} outliers (Cutoff > {limit:,.2f})\")\n",
    "\n",
    "end_time_co = now()\n",
    "\n",
    "\n",
    "# --- DOCUMENTATION ---\n",
    "co_ass_uuid = \"4120be38-c4ad-4f8f-a1cd-b6e6df3011a3\" # UUID for Detection\n",
    "# UPDATED: Extended notes on Categoricals and Ordinals based on visual inspection\n",
    "co_comment = \"\"\"\n",
    "Outlier Findings from Visual Inspection & Detection Strategy:\n",
    "1. ConvertedCompYearly (Target): Visual analysis showed extreme right-skewness with a massive spike near zero and a long tail extending to 50M. IQR is applied to remove these extremes to prevent regression coefficients from being dominated by outliers.\n",
    "2. WorkExp & YearsCode (Features): Histograms revealed a right-skewed distribution. Boxplots identified values > ~40-50 years as statistical outliers. While physically possible, these high-leverage points can skew the slope of linear models and are removed via IQR.\n",
    "3. Note on Categoricals: High cardinality and rare labels were observed in DevType, Country, and Industry (long tails), while MainBranch shows extreme imbalance. These will be handled in the Data Preparation phase via Frequency Thresholding (grouping rare labels into 'Other') or Binary Grouping, rather than statistical outlier removal.\n",
    "4. Note on Ordinals: Low values in JobSat (0-2) are confirmed as valid negative sentiment, not errors, and will be retained. However, non-informative labels found in ordinal columns (e.g., 'I don't know' in OrgSize, 'Prefer not to say' in Age) are flagged for exclusion or imputation during Data Preparation.\n",
    "5. Note on Ranked Features: This has already been done. The columns with highest correlation to the target were retained, while weakly correlated ranking features were dropped. These can stay as is for modeling.\n",
    "\"\"\"\n",
    "\n",
    "report_json = json.dumps(outliers_report).replace('\"', \"'\")\n",
    "\n",
    "check_outliers_activity = [\n",
    "    ':check_outliers rdf:type prov:Activity .',\n",
    "    ':check_outliers sc:isPartOf :data_understanding_phase .',\n",
    "    ':check_outliers rdfs:label \"Check for Outliers (IQR)\" .',\n",
    "    f':check_outliers rdfs:comment \"\"\"{co_comment}\"\"\" .', \n",
    "    f':check_outliers prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    \n",
    "    # Inputs\n",
    "    ':check_outliers prov:used :loaded_survey_data .',\n",
    "\n",
    "    # Executor\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid} .',\n",
    "    f':{co_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # Output Report\n",
    "    ':outlier_report rdf:type prov:Entity .',\n",
    "    ':outlier_report rdfs:label \"Outlier Detection Report\" .',\n",
    "    f':outlier_report rdfs:comment \"\"\"{report_json}\"\"\" .',\n",
    "    ':outlier_report prov:wasGeneratedBy :check_outliers .',\n",
    "]\n",
    "engine.insert(check_outliers_activity, prefixes=prefixes)\n",
    "\n",
    "\n",
    "# --- B. Activity: INSPECT & DECIDE ---\n",
    "ior_ass_uuid = \"3cb16479-de42-4f70-9f4b-3ee25c371db9\" # UUID for Inspection\n",
    "ior_comment = \"\"\"\n",
    "Inspection & Decision Log:\n",
    "1. Numerical Columns (Action: Remove Outliers):\n",
    "   - ConvertedCompYearly, WorkExp, YearsCode: The calculated IQR thresholds successfully isolate extreme skew and high-leverage points (>50 years exp, >50M salary). Decision is to REMOVE these rows immediately to ensure regression stability.\n",
    "\n",
    "2. Ordinal Columns (Action: Filter/Keep in Data Prep):\n",
    "   - JobSat: Low values (0-2) are valid sentiment. Decision: KEEP.\n",
    "   - OrgSize: 'I don't know' provides no utility. Decision: Flag for REMOVAL.\n",
    "   - Age: 'Prefer not to say' is non-informative. Decision: Flag for REMOVAL.\n",
    "\n",
    "3. Categorical Columns (Action: Grouping in Data Prep):\n",
    "   - Country, DevType, Industry, EmploymentAddl: Long tails confirmed. Decision: Apply Frequency Thresholding (Group rare labels into 'Other').\n",
    "   - MainBranch: Extreme imbalance. Decision: Binary Grouping (Pro vs. Non-Pro).\n",
    "\"\"\"\n",
    "\n",
    "inspect_activity = [\n",
    "    ':inspect_outlier_report rdf:type prov:Activity .',\n",
    "    ':inspect_outlier_report sc:isPartOf :data_understanding_phase .',\n",
    "    ':inspect_outlier_report rdfs:label \"Inspect and Decide on Outliers\" .',\n",
    "    f':inspect_outlier_report rdfs:comment \"\"\"{ior_comment}\"\"\" .',\n",
    "    f':inspect_outlier_report prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    \n",
    "    # Input\n",
    "    ':inspect_outlier_report prov:used :outlier_report .',\n",
    "\n",
    "    # Executor\n",
    "    f':inspect_outlier_report prov:qualifiedAssociation :{ior_ass_uuid} .',\n",
    "    f':{ior_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid} prov:agent :{student_a} .',\n",
    "    f':{ior_ass_uuid} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # Output Decision\n",
    "    ':outlier_decision rdf:type prov:Entity .',\n",
    "    ':outlier_decision rdfs:label \"Decision: Remove Num. Outliers & Defer Grouping\" .',\n",
    "    ':outlier_decision rdfs:comment \"Action: Remove IQR outliers for Target/Numerical features. Also important is the removal of non-informative labels (Age/OrgSize) and categorical grouping to Data Preparation phase.\" .',\n",
    "    ':outlier_decision prov:wasGeneratedBy :inspect_outlier_report .',\n",
    "]\n",
    "engine.insert(inspect_activity, prefixes=prefixes)\n",
    "\n",
    "print(\"Outlier detection and decision logic updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea2623",
   "metadata": {},
   "source": [
    "## Manual Qualitative Assessment (Points e, f, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3bb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_activity_id = \"d0c7d6a1-1f6c-4d1e-9b58-0b7d8e4bfebb\"\n",
    "\n",
    "# --- 1. CLEAN TEXT CONTENT ---\n",
    "# We strip out potential problem characters (like LaTeX arrows) and use standard ASCII\n",
    "# The graph doesn't render LaTeX anyway, so standard text is better.\n",
    "\n",
    "ethical_text = \"\"\"\n",
    "e. Ethical & Bias Evaluation:\n",
    "   A major ethical issue is whether broad survey data is suitable for predicting an individual's salary. Surveys look at general statistics and often miss a person's unique situation. If we use this model for real-world pay decisions, we risk unfairly capping the salary of employees who don't fit the \"average\" profile, which could negatively impact their livelihoods.\n",
    "   We also see bias regarding geography and money. The data clusters respondents from countries like Ukraine (using UAH) versus Western nations (using USD or EUR).\n",
    "   If the model doesn't account for the cost of living differences, it might unfairly rate senior developers from these regions as \"low value.\" There is also a risk of ageism because the data lumps everyone over 65 into one group, which might make the model think older people use less technology.\n",
    "   Finally, prioritizing formal degrees like a Master's over self-taught skills could unfairly penalize experienced developers who didn't go to university.\n",
    "\"\"\"\n",
    "\n",
    "risk_text = \"\"\"\n",
    "f. Risks & External Questions:\n",
    "   A fundamental risk is that we cannot verify if survey respondents are telling the truth. Self-reported data often contains exaggerated claims or \"liars,\" and this dishonesty might correlate with specific locations or age groups, creating hidden biases we cannot easily detect.\n",
    "   Financially, the 'CompTotal' column is very risky because it mixes local currenciessome people report in thousands (EUR) and others in millions (UAH). Because these numbers are incomparable, 'CompTotal' will not be part of our final dataset.\n",
    "   We must rely on 'ConvertedCompYearly' instead, but this value is missing for many users (like students), which creates gaps in our data.\n",
    "   Subjective questions also pose a risk; for example, a person's fear of AI might skew their answers about productivity.\n",
    "\"\"\"\n",
    "\n",
    "action_plan_text = \"\"\" g. Preparation Action Plan:\n",
    "    The data preparation strategy begins with removing rows where the target variable ConvertedCompYearly is missing, as imputing salary would introduce unacceptable noise for the business objective.\n",
    "    We will also remove the CompTotal and Currency columns to prevent multicollinearity and magnitude errors once the converted salary is confirmed.\n",
    "    For cleaning, we must filter outliers in YearsCode, converting string descriptions into numeric values.\n",
    "    Imputation will be applied to WorkExp using the median experience of corresponding age groups, and missing values in Likert-scale columns like TechEndorse will be assumed to indicate no endorsement.\n",
    "    Feature engineering will focus on splitting semicolon-delimited strings in LanguageHaveWorkedWith into multi-hot encoded boolean flags.\n",
    "    Finally, we will reduce high cardinality in the Country column by binning nations into broader regions and convert ordinal OrgSize ranges into numeric estimates to facilitate correlation analysis.\n",
    "\n",
    "    Summary:\n",
    "    - Remove Columns on Personal Decisions: So as there are many columns, which are most likely irreleavnt to the target variable, we decided to removed many of them.\n",
    "    - Remove Columns due to Correlation: From these pre selected columns, we further removed columns, which had low correlation with the target variable. Allthough 'YearsCode' and 'WorkExp' had low correlation, we kept them due to their domain relevance. We also kept the Ranking columns despite low correlation to the target, as combined they may provide useful signal.\n",
    "    - Target Cleaning: Remove rows missing 'ConvertedCompYearly' to ensure valid regression targets.\n",
    "    - Outlier Removal: Apply IQR method to 'ConvertedCompYearly' and 'YearsCode' to remove extreme values.\n",
    "    - Imputation: Fill missing 'WorkExp' using age-group medians; set null 'TechEndorse' to 0.\n",
    "    - Changing Ranking Orders: For columns that belong to a ranking group (e.g. TechEndorse_1, TechEndorse_2, ...), we have to inverse the order, so that higher values indicate stronger endorsement.\n",
    "    - Ordinal Encoding: For Ordinal columns like 'OrgSize', map ranges to numeric points that represent the order.\n",
    "    - Norminal Encoding: For Categorical columns like 'Country', reduce cardinality by only using the top N countries and group the rest into \"Others\".\n",
    "    - Mutli-Select Encoding: For Mutli-Select columns like 'LanguageHaveWorkedWith', split semicolon-delimited strings into separate boolean columns for each language.\n",
    "    - Scale: Apply a Scaler to all numeric features to ensure that features with larger magnitudes do not disproportionately influence the Linear Regression weights.\n",
    "\"\"\"\n",
    "\n",
    "# Serialize to safe JSON strings (adds quotes and escapes newlines automatically)\n",
    "# This prevents syntax errors in the SPARQL query.\n",
    "ethical_safe = json.dumps(ethical_text)[1:-1] # [1:-1] removes the outer quotes added by json.dumps\n",
    "risk_safe = json.dumps(risk_text)[1:-1]\n",
    "action_safe = json.dumps(action_plan_text)[1:-1]\n",
    "\n",
    "\n",
    "# --- 2. GENERATE NEW ID ---\n",
    "current_time = now()\n",
    "\n",
    "assess_query = [\n",
    "    f':{assess_activity_id} rdf:type prov:Activity .',\n",
    "    f':{assess_activity_id} rdfs:label \"Qualitative Assessment\" .',\n",
    "    f':{assess_activity_id} sc:isPartOf :data_understanding_phase .',\n",
    "    f':{assess_activity_id} prov:wasAssociatedWith :{student_a} .',\n",
    "    f':{assess_activity_id} prov:startedAtTime \"{current_time}\"^^xsd:dateTime .',\n",
    "    \n",
    "    # Point e: Ethics\n",
    "    f':report_ethics_{assess_activity_id} rdf:type prov:Entity .',\n",
    "    f':report_ethics_{assess_activity_id} rdfs:comment \"{ethical_safe}\" .', # Use single quotes with safe text\n",
    "    f':report_ethics_{assess_activity_id} prov:wasGeneratedBy :{assess_activity_id} .',\n",
    "\n",
    "    # Point f: Risks\n",
    "    f':report_risks_{assess_activity_id} rdf:type prov:Entity .',\n",
    "    f':report_risks_{assess_activity_id} rdfs:comment \"{risk_safe}\" .',\n",
    "    f':report_risks_{assess_activity_id} prov:wasGeneratedBy :{assess_activity_id} .',\n",
    "\n",
    "    # Point g: Action Plan\n",
    "    f':report_actions_{assess_activity_id} rdf:type prov:Entity .',\n",
    "    f':report_actions_{assess_activity_id} rdfs:comment \"{action_safe}\" .',\n",
    "    f':report_actions_{assess_activity_id} prov:wasGeneratedBy :{assess_activity_id} .',\n",
    "]\n",
    "\n",
    "engine.insert(assess_query, prefixes=prefixes)\n",
    "print(f\" Qualitative Assessment logged with ID: {assess_activity_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16349e3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce570db",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d290a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "f':data_preparation_phase rdf:type prov:Activity .',\n",
    "f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .', \n",
    "]\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d076f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_outliers_code_writer = student_b\n",
    "def handle_outliers(df:pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    # REMOVE OUTLIERS\n",
    "    return df\n",
    "\n",
    "start_time_td = now()\n",
    "handle_outliers(data, outliers_report)\n",
    "end_time_td = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# This is the continuation of the example from the Data Understanding phase above.\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => already done in data understanding phase\n",
    "# 2. activity inspects the outcome and derives decisions => already done in data understanding phase\n",
    "# 3. activity follows up on the decision by changing the data => in this case by removing the the outliers that were found\n",
    "\n",
    "ro_ass_uuid_executor = \"ec7e81e1-86ea-475a-a8d4-c7d8ee535488\"\n",
    "handle_outliers_executor = [\n",
    "    f':handle_outliers prov:qualifiedAssociation :{ro_ass_uuid_executor} .',\n",
    "    f':{ro_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ro_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ro_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(handle_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "td_ass_uuid_writer = \"1405f15a-3545-4014-a962-637f3c10a137\"\n",
    "td_comment = \"\"\"\n",
    "Removing all outliers that were identifying in the Data Understanding Phase.\n",
    "\"\"\"\n",
    "handle_outliers_activity = [\n",
    "    ':handle_outliers rdf:type prov:Activity .',\n",
    "    ':handle_outliers sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_outliers rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':handle_outliers rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':handle_outliers prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:qualifiedAssociation :{td_ass_uuid_writer} .',\n",
    "    f':{td_ass_uuid_writer} prov:agent :{handle_outliers_code_writer} .',\n",
    "    f':{td_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{td_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_outliers prov:used :data .',\n",
    "    ':handle_outliers prov:used :outlier_decision .',\n",
    "    ':cleaned_data rdf:type prov:Entity .',\n",
    "    ':cleaned_data prov:wasGeneratedBy :handle_outliers .',\n",
    "    ':cleaned_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(handle_outliers_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100cff7-8fd5-4ba1-8913-b4f1ccdfda35",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8800ce26b8f3e2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Continue with other tasks of the Data Preparation phase such as binning, scaling etc...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843451cc",
   "metadata": {},
   "source": [
    "### Performing pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd279ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is used only for the creation and afterwards copying uuids in order to be able to fullfill the provenance.\n",
    "import uuid\n",
    "print(str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting unnecessary columns:\n",
    "cols_to_remove = columns_to_drop + cols_to_drop_final + cols_to_drop_from_schema\n",
    "existing_cols = [col for col in cols_to_remove if col in data.columns]\n",
    "\n",
    "# Drop all columns that are in \"data\" but not in \"df_analysis\"\n",
    "cols_to_remove = [col for col in data.columns if col not in df_analysis.columns]\n",
    "processed_data = data.drop(columns=cols_to_remove)\n",
    "print(\"Remaining columns after removal:\")\n",
    "print(processed_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb888c1",
   "metadata": {},
   "source": [
    "#### 1. Missing Values in Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_code_writer = student_b\n",
    "#processed_data = data\n",
    "\n",
    "def handleNulls(df:pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    #df = pd.read_csv('data/stack-overflow-developer-survey-2025/survey_results_public.csv')\n",
    "    rowsWithoutNull = df['ConvertedCompYearly'].notna().sum()\n",
    "\n",
    "    print(\"Rows without null Value in ConvertedCompYearly: \" + str(rowsWithoutNull))\n",
    "    cleanedDataset = df.dropna(subset=['ConvertedCompYearly'])\n",
    "    cleanedDataset.to_csv(\"Mav_TestData/output.csv\", index=False)\n",
    "    print(\"Finished deleting rows with NaN values\")\n",
    "    \n",
    "    return cleanedDataset\n",
    "\n",
    "def checkForNaN(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfNew = df.copy()\n",
    "\n",
    "    cols = [\"WorkExp\", \"YearsCode\"]\n",
    "\n",
    "    for col in cols:\n",
    "        median_value = dfNew[col].median()\n",
    "        dfNew[col] = dfNew[col].fillna(median_value)\n",
    "\n",
    "    return dfNew\n",
    "\n",
    "\n",
    "\n",
    "start_time_td = now()\n",
    "#handleNulls(data, outliers_report)\n",
    "processed_data = handleNulls(processed_data, outliers_report)\n",
    "processed_data = checkForNaN(data)\n",
    "end_time_td = now()\n",
    "\n",
    "\n",
    "null_value_uuid_executor = \"6ec9c276-2320-47bb-aa02-5e250434fd78\"\n",
    "null_values_executor = [\n",
    "    f':handle_missing_target_values prov:qualifiedAssociation :{null_value_uuid_executor} .',\n",
    "    f':{null_value_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{null_value_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{null_value_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(null_values_executor, prefixes=prefixes)\n",
    "\n",
    "print(\"Inserted executor\")\n",
    "\n",
    "null_value_uuid_writer = \"29549238-43ae-470b-b427-b98b9796ac4d\"\n",
    "td_comment = \"\"\"\n",
    "Removed all NULL / NaN values in ConvertedCompYearly, as we need these values in order to calculated the average income\n",
    "\"\"\"\n",
    "null_values_activity = [\n",
    "    ':handle_missing_target_values rdf:type prov:Activity .',\n",
    "    ':handle_missing_target_values sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_missing_target_values rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':handle_missing_target_values rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':handle_missing_target_values prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_missing_target_values prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_missing_target_values prov:qualifiedAssociation :{null_value_uuid_writer} .',\n",
    "    f':{null_value_uuid_writer} prov:agent :{null_values_code_writer} .',\n",
    "    f':{null_value_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{null_value_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_missing_target_values prov:used :processed_data .',\n",
    "    ':data_no_missing_target  rdf:type prov:Entity .',\n",
    "    ':data_no_missing_target rdfs:label \"Dataset without missing target values in ConvertedCompYearly\" .',\n",
    "    ':data_no_missing_target prov:wasGeneratedBy :handle_missing_target_values .',\n",
    "    ':data_no_missing_target prov:wasDerivedFrom :processed_data .',\n",
    "]\n",
    "engine.insert(null_values_activity, prefixes=prefixes)\n",
    "\n",
    "print(\"Inserted writer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863d298",
   "metadata": {},
   "source": [
    "#### 2. Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot from student a from above --> just taken in order to not always scrole up\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(processed_data['ConvertedCompYearly'].dropna(), bins=50, color='#d62728', alpha=0.7)\n",
    "plt.yscale('log')\n",
    "plt.title(\"Salary Distribution (Log Scale)\\n(Notice the isolated bars at $50M+)\")\n",
    "plt.xlabel(\"Annual Salary (USD)\")\n",
    "plt.ylabel(\"Frequency (Log)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ConvertedCompYearly outliers \n",
    "\n",
    "### ATTENTION: I am create a copied dataset here in order to not mess with the original one while still testing the code ###\n",
    "handle_outliers_code_writer = student_b\n",
    "def removeOutliers(df: pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    dfNew = df.copy()\n",
    "\n",
    "    #Just for validation purposes once before and once after performing the deletion of rows by the 0,99 perceptile, to see the differences as well \n",
    "    topEarners = dfNew.sort_values(by='ConvertedCompYearly', ascending=False).head(10)\n",
    "    print(topEarners[['ConvertedCompYearly']])\n",
    "\n",
    "    p99 = dfNew['ConvertedCompYearly'].quantile(0.99)  \n",
    "    dfNew = dfNew[dfNew['ConvertedCompYearly'] <= p99]\n",
    "\n",
    "    topEarners = dfNew.sort_values(by='ConvertedCompYearly', ascending=False).head(10)\n",
    "    print(topEarners[['ConvertedCompYearly']])\n",
    "\n",
    "    #Taken from studen_a\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(dfNew['ConvertedCompYearly'].dropna(), bins=50, color='#d62728', alpha=0.7)\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Salary Distribution (Log Scale)\\n(After Removing Top 3 Outliers)\")\n",
    "    plt.xlabel(\"Annual Salary (USD)\")\n",
    "    plt.ylabel(\"Frequency (Log)\")\n",
    "\n",
    "    return dfNew\n",
    "\n",
    "\n",
    "start_time_td = now()\n",
    "processed_data = removeOutliers(processed_data, outliers_report)\n",
    "end_time_td = now()\n",
    "\n",
    "outlier_remover_uuid_executor = \"f41b9543-cf1e-403b-9bcd-00ddd0577055\"\n",
    "outlier_remover_executor = [\n",
    "    f':handle_outliers prov:qualifiedAssociation :{outlier_remover_uuid_executor} .',\n",
    "    f':{outlier_remover_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{outlier_remover_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{outlier_remover_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(outlier_remover_executor, prefixes=prefixes)\n",
    "\n",
    "print(\"Inserted executor\")\n",
    "\n",
    "outlier_remover_uuid_writer = \"c6b8ac8d-6662-4b90-8913-1846063a3211\"\n",
    "td_comment = \"\"\"\n",
    "Removed the outliers with the help of the 0,99 perceptile\n",
    "\"\"\"\n",
    "\n",
    "outlier_remover_activity = [\n",
    "    ':handle_outliers rdf:type prov:Activity .',\n",
    "    ':handle_outliers rdfs:label \"Remove extreme salary outliers above 99th percentile\" .',\n",
    "    ':handle_outliers sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_outliers rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':handle_outliers rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':handle_outliers prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:qualifiedAssociation :{outlier_remover_uuid_writer} .',\n",
    "    f':{outlier_remover_uuid_writer} prov:agent :{handle_outliers_code_writer} .',\n",
    "    f':{outlier_remover_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{outlier_remover_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_outliers prov:used :processed_data .',\n",
    "    ':cleaned_data_after_outliers rdf:type prov:Entity .',\n",
    "    ':cleaned_data_after_outliers rdfs:label \"Dataset after removing extreme salary outliers\" .',\n",
    "    ':cleaned_data_after_outliers prov:wasGeneratedBy :handle_outliers .',\n",
    "    ':cleaned_data_after_outliers prov:wasDerivedFrom :processed_data .',\n",
    "]\n",
    "engine.insert(outlier_remover_activity, prefixes=prefixes)\n",
    "\n",
    "print(\"Inserted writer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5522a",
   "metadata": {},
   "source": [
    "#### 3. Inverted Logic in Rnaking Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverter_code_writer = student_b\n",
    "def invertRanking(df: pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    #The following type of doing the invertion of the values may not be the best one, but as I am not the best of python programmers, it will hopefully be enough.\n",
    "    mapping = {\n",
    "        1: 14,\n",
    "        2: 13,\n",
    "        3: 12,\n",
    "        4: 11,\n",
    "        5: 10,\n",
    "        6: 9,\n",
    "        7: 8,\n",
    "        8: 7,\n",
    "        9: 6,\n",
    "        10: 5,\n",
    "        11: 4,\n",
    "        12: 3,\n",
    "        13: 2,\n",
    "        14: 1\n",
    "    }\n",
    "\n",
    "    dfNew = df.copy()\n",
    "    techEndorsCols = [\"TechEndorse_4\", \"TechEndorse_6\", \"TechEndorse_8\"]\n",
    "    jobsatCols = [\"JobSatPoints_4\", \"JobSatPoints_11\", \"JobSatPoints_6\"]\n",
    "    techOpposeCols = [\"TechOppose_3\", \"TechOppose_9\", \"TechOppose_13\"]\n",
    "\n",
    "    columnsToInvert = techEndorsCols + jobsatCols + techOpposeCols +  [\"JobSat\"]\n",
    "\n",
    "    for col in columnsToInvert:\n",
    "        dfNew[col] = dfNew[col].map(mapping).fillna(0)\n",
    "\n",
    "    return dfNew\n",
    "\n",
    "def printRankings(df, n=5):\n",
    "    #I've just taken some random samples in order to check if everything works as inteded.\n",
    "    cols_to_show = [\"TechEndorse_4\", \"JobSatPoints_11\", \"TechOppose_13\", \"JobSat\"]\n",
    "\n",
    "    print(df[cols_to_show].head(n))\n",
    "\n",
    "printRankings(processed_data)\n",
    "start_time_td = now()\n",
    "processed_data = invertRanking(processed_data, outliers_report)\n",
    "end_time_td = now()\n",
    "printRankings(processed_data)\n",
    "\n",
    "inverter_uuid_executor = \"741afcd4-1e37-416c-b8ab-6516c5c55a7f\"\n",
    "inverter_executor = [\n",
    "    f':invert_logic_for_ranking prov:qualifiedAssociation :{inverter_uuid_executor} .',\n",
    "    f':{inverter_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{inverter_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{inverter_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(inverter_executor, prefixes=prefixes)\n",
    "\n",
    "print(\"Inserted executor\")\n",
    "\n",
    "inverter_uuid_writer = \"1d033b92-a730-4fa3-a1ea-6df3e2d3681c\"\n",
    "td_comment = \"\"\"\n",
    "Inverted all columns with TechEndors_*, JobSatPoints_* and JobSat in order to have it perfectly fit for the regression model.\n",
    "\"\"\"\n",
    "\n",
    "inverter_activity = [\n",
    "    ':invert_logic_for_ranking rdf:type prov:Activity .',\n",
    "    ':invert_logic_for_ranking rdfs:label \"Invert ranking features\" .',\n",
    "    ':invert_logic_for_ranking sc:isPartOf :data_preparation_phase .',\n",
    "    ':invert_logic_for_ranking rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':invert_logic_for_ranking rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':invert_logic_for_ranking prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':invert_logic_for_ranking prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':invert_logic_for_ranking prov:qualifiedAssociation :{inverter_uuid_writer} .',\n",
    "    f':{inverter_uuid_writer} prov:agent :{inverter_code_writer} .',\n",
    "    f':{inverter_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{inverter_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':invert_logic_for_ranking prov:used :cleaned_data_after_outliers .',\n",
    "    ':data_after_invert_logic_for_ranking rdf:type prov:Entity .',\n",
    "    ':data_after_invert_logic_for_ranking rdfs:label \"dtaaset with inverted ranking features\" .',\n",
    "    ':data_after_invert_logic_for_ranking prov:wasGeneratedBy :invert_logic_for_ranking .',\n",
    "    ':data_after_invert_logic_for_ranking prov:wasDerivedFrom :cleaned_data_after_outliers .',\n",
    "]\n",
    "engine.insert(inverter_activity, prefixes=prefixes)\n",
    "print(\"Inserted writer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b4302",
   "metadata": {},
   "source": [
    "#### 4. Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age types and how they gonna be mapped\n",
    "#Under 18 years old --> 1\n",
    "#18-24 years old --> 2\n",
    "#25-34 years old --> 3\n",
    "#35-44 years old --> 3\n",
    "#45-54 years old --> 4\n",
    "#55-64 years old --> 5\n",
    "#65 years or older --> 6\n",
    "#Prefer not to say --> 0\n",
    "#NaN --> 0\n",
    "\n",
    "def ageMapping(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfNew = df.copy()\n",
    "\n",
    "    mapping = {\n",
    "        \"Under 18 years old\": 1,\n",
    "        \"18-24 years old\": 2,\n",
    "        \"25-34 years old\": 3,\n",
    "        \"35-44 years old\": 3,\n",
    "        \"45-54 years old\": 4,\n",
    "        \"55-64 years old\": 5,\n",
    "        \"65 years or older\": 6,\n",
    "        \"Prefer not to say\": 0\n",
    "    }\n",
    "    \n",
    "    dfNew['AgeMapped'] = dfNew['Age'].map(mapping).fillna(0).astype(int)\n",
    "    \n",
    "    return dfNew\n",
    "\n",
    "# Education levels and how they gonna be mapped\n",
    "#Primary/elementary school --> 1\n",
    "#Secondary school (e.g. American high school, German Realschule orGymnasium, etc.) --> 2\n",
    "#Some college/university study without earning a degree --> 3\n",
    "#Associate degree (A.A., A.S., etc.) --> 4\n",
    "#Bachelors degree (B.A., B.S., B.Eng., etc.) --> 5\n",
    "#Masters degree (M.A., M.S., M.Eng., MBA, etc.) --> 6\n",
    "#Professional degree (JD, MD, Ph.D, Ed.D, etc.) --> 7\n",
    "# else or NaN --> 0\n",
    "\n",
    "def educationMapping(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfNew = df.copy()\n",
    "\n",
    "    mapping = {\n",
    "        \"Under 18 years old\": 1,\n",
    "        \"Primary/elementary school\": 1,\n",
    "        \"Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)\": 2,\n",
    "        \"Some college/university study without earning a degree\": 3,\n",
    "        \"Associate degree (A.A., A.S., etc.)\": 4,\n",
    "        \"Bachelors degree (B.A., B.S., B.Eng., etc.)\": 5,\n",
    "        \"Masters degree (M.A., M.S., M.Eng., MBA, etc.)\": 6,\n",
    "        \"Professional degree (JD, MD, Ph.D, Ed.D, etc.)\": 7,\n",
    "    }\n",
    "\n",
    "    dfNew[\"EducationLevelMapped\"] = dfNew[\"EdLevel\"].map(mapping).fillna(0).astype(int)\n",
    "\n",
    "    return dfNew\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# ORgSIzes and how they gonna be mapped\n",
    "# Just me - I am a freelancer, sole proprietor, etc. --> 1\n",
    "# Less than 20 employees --> 2\n",
    "# 20 to 99 employees --> 3\n",
    "# 100 to 499 employees --> 4\n",
    "# 500 to 999 employees --> 5\n",
    "# 1,000 to 4,999 employees --> 6\n",
    "# 5,000 to 9,999 employees --> 7\n",
    "# 10,000 or more employees --> 8\n",
    "# I dont know --> 0\n",
    "\n",
    "def mapOrgSize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfNew = df.copy()\n",
    "\n",
    "    mapping = {\n",
    "        \"Just me - I am a freelancer, sole proprietor, etc.\": 1,\n",
    "        \"Less than 20 employees\": 2,\n",
    "        \"20 to 99 employees\": 3,\n",
    "        \"100 to 499 employees\": 4,\n",
    "        \"500 to 999 employees\": 5,\n",
    "        \"1,000 to 4,999 employees\": 6,\n",
    "        \"5,000 to 9,999 employees\": 7,\n",
    "        \"10,000 or more employees\": 8,\n",
    "        \"I dont know\": 0,\n",
    "    }\n",
    "\n",
    "    dfNew['OrgSizeMapped'] = dfNew['OrgSize'].map(mapping).fillna(0).astype(int)\n",
    "\n",
    "    return dfNew\n",
    "\n",
    "# AI usage levels and how they gonna be mapped\n",
    "#Yes, I use AI tools daily --> 4\n",
    "#Yes, I use AI tools weekly --> 3\n",
    "#Yes, I use AI tools monthly or infrequently --> 2\n",
    "#No, but I plan to soon --> 1\n",
    "#No, and I don't plan to --> 0\n",
    "\n",
    "def mapAIUsage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfNew = df.copy()\n",
    "\n",
    "    mapping = {\n",
    "        \"Yes, I use AI tools daily\": 4,\n",
    "        \"Yes, I use AI tools weekly\": 3,\n",
    "        \"Yes, I use AI tools monthly or infrequently\": 2,\n",
    "        \"No, but I plan to soon\": 1,\n",
    "        \"No, and I don't plan to\": 0\n",
    "    }\n",
    "    \n",
    "    dfNew['AISelectMapped'] = dfNew['AISelect'].map(mapping).fillna(0).astype(int)\n",
    "    \n",
    "    return dfNew\n",
    "# AI sent# levels and how they gonna be mapped\n",
    "#Very favorable --> 4\n",
    "#Favorable --> 3\n",
    "#Indifferent --> 2\n",
    "#Unfavorable --> 1\n",
    "#Very unfavorable --> 0\n",
    "#Unsure --> 0\n",
    "\n",
    "def mapAISent(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfNew = df.copy()\n",
    "\n",
    "    mapping = {\n",
    "        \"Very favorable\": 4,\n",
    "        \"Favorable\": 3,\n",
    "        \"Indifferent\": 2,\n",
    "        \"Unfavorable\": 1,\n",
    "        \"Very unfavorable\": 0,\n",
    "        \"Unsure\": 0\n",
    "    }\n",
    "\n",
    "    dfNew['AISentMapped'] = dfNew['AISent'].map(mapping).fillna(0).astype(int)\n",
    "\n",
    "    return dfNew\n",
    "\n",
    "\n",
    "def printOrdinalEncoding(df, n=7):\n",
    "    cols_to_show = [\"Age\", \"AgeMapped\", \"EdLevel\", \"EducationLevelMapped\", \"OrgSize\", \"OrgSizeMapped\", \"AISelect\", \"AISelectMapped\", \"AISent\", \"AISentMapped\"]\n",
    "\n",
    "    print(df[cols_to_show].head(n))\n",
    "\n",
    "\n",
    "\n",
    "def processAllOrdinalEncoding(df: pd.DataFrame,  outliers_report: dict) -> pd.DataFrame:\n",
    "    processed_data_temp = ageMapping(processed_data)\n",
    "    processed_data_temp = educationMapping(processed_data_temp)\n",
    "    processed_data_temp = mapOrgSize(processed_data_temp)\n",
    "    processed_data_temp = mapAIUsage(processed_data_temp)\n",
    "    processed_data_temp = mapAISent(processed_data_temp)\n",
    "\n",
    "    return processed_data_temp\n",
    "\n",
    "start_time_td = now()\n",
    "processed_data = processAllOrdinalEncoding(processed_data, outliers_report)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "printOrdinalEncoding(processed_data)\n",
    "end_time_td = now()\n",
    "\n",
    "ordinal_code_writer = student_b\n",
    "\n",
    "ordinal_uuid_executor = \"047ef8ee-059d-450f-b6d9-b53a0fd024f0\"\n",
    "ordinal_executor = [\n",
    "    f':transform_data_from_ordinal_encoding prov:qualifiedAssociation :{ordinal_uuid_executor} .',\n",
    "    f':{ordinal_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ordinal_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ordinal_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(ordinal_executor, prefixes=prefixes)\n",
    "\n",
    "print(\"Inserted executor\")\n",
    "\n",
    "ordinal_uuid_writer = \"b23a21dd-f992-4c77-8976-81ae9594bc96\"\n",
    "td_comment = \"\"\"\n",
    "transformed ordinal encoding in order to have one column for each possible value which is then ordinal encoded with values 1 to n\n",
    "\"\"\"\n",
    "\n",
    "ordinal_activity = [\n",
    "    ':transform_data_from_ordinal_encoding rdf:type prov:Activity .',\n",
    "    ':transform_data_from_ordinal_encoding rdfs:label \"Transformed ordinal encoding\" .',\n",
    "    ':transform_data_from_ordinal_encoding sc:isPartOf :data_preparation_phase .',\n",
    "    ':transform_data_from_ordinal_encoding rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':transform_data_from_ordinal_encoding rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':transform_data_from_ordinal_encoding prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':transform_data_from_ordinal_encoding prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':transform_data_from_ordinal_encoding prov:qualifiedAssociation :{ordinal_uuid_writer} .',\n",
    "    f':{ordinal_uuid_writer} prov:agent :{ordinal_code_writer} .',\n",
    "    f':{ordinal_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ordinal_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':transform_data_from_ordinal_encoding prov:used :data_after_invert_logic_for_ranking .',\n",
    "    ':data_after_transform_data_from_ordinal_encoding rdf:type prov:Entity .',\n",
    "    ':data_after_transform_data_from_ordinal_encoding rdfs:label \"dataset after ordinal encoding\" .',\n",
    "    ':data_after_transform_data_from_ordinal_encoding prov:wasGeneratedBy :transform_data_from_ordinal_encoding .',\n",
    "    ':data_after_transform_data_from_ordinal_encoding prov:wasDerivedFrom :data_after_invert_logic_for_ranking .',\n",
    "]\n",
    "engine.insert(ordinal_activity, prefixes=prefixes)\n",
    "print(\"Inserted writer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773c2d0",
   "metadata": {},
   "source": [
    "#### 5 Multi-Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ec915",
   "metadata": {},
   "outputs": [],
   "source": [
    "programmingLanguages = [\"Ada\", \"Assembly\", \"Bash/Shell (allshells)\", \"C\", \"C#\", \"C++\", \"COBOL\", \"Dart\", \"Delphi\", \"Elixir\", \"Erlang\", \"F#\", \"Fortran\", \"GDScript\", \"Go\", \"Groovy\", \"HTML/CSS\", \"Java\", \"JavaScript\", \"Kotlin\", \"Lisp\", \"Lua\", \"MATLAB\", \"MicroPython\", \"OCaml\", \"Perl\", \"PHP\", \"PowerShell\", \"Prolog\", \"Python\", \"R\", \"Ruby\", \"Rust\", \"Scala\", \"SQL\", \"Swift\", \"TypeScript\", \"VBA\", \"Visual Basic (.Net)\", \"Zig\", \"Mojo\", \"Gleam\",]\n",
    "databases = [\"BigQuery\", \"Cassandra\", \"Cloud Firestore\", \"Cosmos DB\", \"Databricks SQL\", \"Datomic\", \"DuckDB\", \"Dynamodb\", \"Elasticsearch\", \"Firebase Realtime Database\", \"H2\", \"IBM DB2\", \"InfluxDB\", \"MariaDB\", \"Microsoft Access\", \"Microsoft SQL Server\", \"MongoDB\", \"MySQL\", \"Neo4J\", \"Oracle\", \"PostgreSQL\", \"Redis\", \"Snowflake\", \"SQLite\", \"Supabase\", \"Clickhouse\", \"Cockroachdb\", \"Amazon Redshift\", \"Pocketbase\", \"Valkey\"]\n",
    "cloudPlattforms = [\"Amazon Web Services (AWS)\", \"Ansible\", \"APT\", \"Bun\", \"Cargo\", \"Chocolatey\", \"Cloudflare\", \"Composer\", \"Datadog\", \"Digital Ocean\", \"Docker\", \"Firebase\", \"Google Cloud\", \"Gradle\", \"Heroku\", \"Homebrew\", \"IBM Cloud\", \"Kubernetes\", \"Make\", \"Maven (build tool)\", \"Microsoft Azure\", \"MSBuild\", \"Netlify\", \"New Relic\", \"Ninja\", \"npm\", \"NuGet\", \"Pacman\", \"Pip\", \"pnpm\", \"Podman\", \"Poetry\", \"Prometheus\", \"Railway\", \"Splunk\", \"Supabase\", \"Terraform\", \"Vercel\", \"Vite\", \"Webpack\", \"Yandex Cloud\", \"Yarn\"]\n",
    "webFrameworks = [\"Angular\", \"AngularJS\", \"ASP.NET\", \"ASP.NET Core\", \"Astro\", \"Blazor\", \"Deno\", \"Django\", \"Drupal\", \"Express\", \"FastAPI\", \"Fastify\", \"Flask\", \"jQuery\", \"Laravel\", \"NestJS\", \"Next.js\", \"Node.js\", \"Nuxt.js\", \"Phoenix\", \"React\", \"Ruby on Rails\", \"Spring Boot\", \"Svelte\", \"Symfony\", \"Vue.js\", \"WordPress\", \"Axum\"]\n",
    "developmentEnvironments = [\"Aider\", \"Android Studio\", \"Bolt\", \"Claude Code\", \"Cline and/or Roo\", \"Cursor\", \"Eclipse\", \"IntelliJ IDEA\", \"JupyterNotebook/JupyterLab\", \"Lovable.dev\", \"Nano\", \"Neovim\", \"Notepad++\", \"PhpStorm\", \"PyCharm\", \"Rider\", \"RustRover\", \"Sublime Text\", \"Trae\", \"Vim\", \"Visual Studio\", \"Visual Studio Code\", \"VSCodium\", \"WebStorm\", \"Windsurf\", \"Xcode\", \"Zed\"]\n",
    "operatingSystems = [\"Windows\", \"MacOS\", \"Ubuntu\", \"Android\", \"Windows Subsystem for Linux (WSL)\", \"iOS\", \"Debian\", \"Linux (non-WSL)\", \"Arch\", \"iPadOS\", \"Red Hat\", \"Fedora\", \"ChromeOS\", \"Pop!_OS\", \"NixOS\"]\n",
    "additionalEmployments = [\"Attending school (full-time)\", \"Attending school (part-time)\", \"Engaged in paid work (less than 10 hours per week)\", \"Engaged in paid work (10-19 hours per week)\", \"Engaged in paid work (20-29 hours per week)\", \"Transitioning to retirement (gradually reducing work hours)\", \"Volunteering (regularly)\", \"Caring for dependents (children, elderly, etc.)\", \"None of the above\"]\n",
    "\n",
    "\n",
    "def addColumns(df: pd.DataFrame,) -> pd.DataFrame:\n",
    "    allColumns = programmingLanguages + databases + cloudPlattforms + webFrameworks + developmentEnvironments + operatingSystems + additionalEmployments\n",
    "\n",
    "    for column in allColumns:\n",
    "        if column not in df.columns:\n",
    "            df[column] = 0\n",
    "    return df\n",
    "\n",
    "processed_data = addColumns(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dbbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def programmingLanguagesToColumns(df: pd.DataFrame, source_col: str = \"LanguageHaveWorkedWith\", sep: str = \";\") -> pd.DataFrame:\n",
    "    cleaned = (\n",
    "        df[source_col].fillna(\"\").astype(str).apply(lambda s: \";\".join([part.strip() for part in s.split(sep) if part.strip()]))\n",
    "    )\n",
    "\n",
    "    dummies = cleaned.str.get_dummies(sep=sep)\n",
    "\n",
    "    for lang in programmingLanguages:\n",
    "        if lang in dummies.columns:\n",
    "            df[lang] = dummies[lang].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def databasesToColumns(df: pd.DataFrame, source_col: str = \"DatabaseHaveWorkedWith\", sep: str = \";\") -> pd.DataFrame:\n",
    "    cleaned = (\n",
    "        df[source_col].fillna(\"\").astype(str).apply(lambda s: \";\".join([part.strip() for part in s.split(sep) if part.strip()]))\n",
    "    )\n",
    "\n",
    "    dummies = cleaned.str.get_dummies(sep=sep)\n",
    "\n",
    "    for lang in databases:\n",
    "        if lang in dummies.columns:\n",
    "            df[lang] = dummies[lang].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def cloudPlattformsToColumns(df: pd.DataFrame, source_col: str = \"PlatformHaveWorkedWith\", sep: str = \";\") -> pd.DataFrame:\n",
    "    cleaned = (\n",
    "        df[source_col].fillna(\"\").astype(str).apply(lambda s: \";\".join([part.strip() for part in s.split(sep) if part.strip()]))\n",
    "    )\n",
    "\n",
    "    dummies = cleaned.str.get_dummies(sep=sep)\n",
    "\n",
    "    for lang in cloudPlattforms:\n",
    "        if lang in dummies.columns:\n",
    "            df[lang] = dummies[lang].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def webFrameworksToColumns(df: pd.DataFrame, source_col: str = \"WebframeHaveWorkedWith\", sep: str = \";\") -> pd.DataFrame:\n",
    "    cleaned = (\n",
    "        df[source_col].fillna(\"\").astype(str).apply(lambda s: \";\".join([part.strip() for part in s.split(sep) if part.strip()]))\n",
    "    )\n",
    "\n",
    "    dummies = cleaned.str.get_dummies(sep=sep)\n",
    "\n",
    "    for lang in webFrameworks:\n",
    "        if lang in dummies.columns:\n",
    "            df[lang] = dummies[lang].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def developmentEnvironmentsToColumns(df: pd.DataFrame, source_col: str = \"DevEnvsHaveWorkedWith\", sep: str = \";\") -> pd.DataFrame:\n",
    "    cleaned = (\n",
    "        df[source_col].fillna(\"\").astype(str).apply(lambda s: \";\".join([part.strip() for part in s.split(sep) if part.strip()]))\n",
    "    )\n",
    "\n",
    "    dummies = cleaned.str.get_dummies(sep=sep)\n",
    "\n",
    "    for lang in developmentEnvironments:\n",
    "        if lang in dummies.columns:\n",
    "            df[lang] = dummies[lang].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def operatingSystemsToColumns(df: pd.DataFrame, source_col: str = \"OpSysProfessional use\", sep: str = \";\") -> pd.DataFrame:\n",
    "    cleaned = (\n",
    "        df[source_col].fillna(\"\").astype(str).apply(lambda s: \";\".join([part.strip() for part in s.split(sep) if part.strip()]))\n",
    "    )\n",
    "\n",
    "    dummies = cleaned.str.get_dummies(sep=sep)\n",
    "\n",
    "    for lang in operatingSystems:\n",
    "        if lang in dummies.columns:\n",
    "            df[lang] = dummies[lang].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def additionalEmploymentsToColumns(df: pd.DataFrame, source_col: str = \"EmploymentAddl\", sep: str = \";\") -> pd.DataFrame:\n",
    "    cleaned = (\n",
    "        df[source_col].fillna(\"\").astype(str).apply(lambda s: \";\".join([part.strip() for part in s.split(sep) if part.strip()]))\n",
    "    )\n",
    "\n",
    "    dummies = cleaned.str.get_dummies(sep=sep)\n",
    "\n",
    "    for lang in additionalEmployments:\n",
    "        if lang in dummies.columns:\n",
    "            df[lang] = dummies[lang].astype(int)\n",
    "\n",
    "    return df\n",
    "def processAllMultiSelectData(df: pd.DataFrame,  outliers_report: dict) -> pd.DataFrame:\n",
    "    processed_data_temp = programmingLanguagesToColumns(processed_data)\n",
    "    processed_data_temp = databasesToColumns(processed_data_temp)\n",
    "    processed_data_temp = cloudPlattformsToColumns(processed_data_temp)\n",
    "    processed_data_temp = webFrameworksToColumns(processed_data_temp)\n",
    "    processed_data_temp = developmentEnvironmentsToColumns(processed_data_temp)\n",
    "    processed_data_temp = operatingSystemsToColumns(processed_data_temp)\n",
    "    processed_data_temp = additionalEmploymentsToColumns(processed_data_temp)\n",
    "\n",
    "    return processed_data_temp\n",
    "\n",
    "start_time_td = now()\n",
    "processed_data = processAllMultiSelectData(processed_data, outliers_report)\n",
    "end_time_td = now()\n",
    "\n",
    "multi_select_code_writer = student_b\n",
    "\n",
    "multi_select_uuid_executor = \"574dc9ed-a79a-4106-9f49-319be6f0b022\"\n",
    "multi_select_executor = [\n",
    "    f':transform_data_from_multi_select prov:qualifiedAssociation :{multi_select_uuid_executor} .',\n",
    "    f':{multi_select_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{multi_select_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{multi_select_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(multi_select_executor, prefixes=prefixes)\n",
    "\n",
    "print(\"Inserted executor\")\n",
    "\n",
    "multi_select_uuid_writer = \"f69e60b5-d103-4c0c-88fd-799bd0fd4de0\"\n",
    "td_comment = \"\"\"\n",
    "transformed multi-select similar to ordinal encoding. We have one column for each possible value which is then binary encoded with values 1 or 0. \n",
    "For non existent values like NaN, a developer got a 0 in every column. \n",
    "\"\"\"\n",
    "\n",
    "multi_select_activity = [\n",
    "    ':transform_data_from_multi_select rdf:type prov:Activity .',\n",
    "    ':transform_data_from_multi_select rdfs:label \"Transform multi-select datata\" .',\n",
    "    ':transform_data_from_multi_select sc:isPartOf :data_preparation_phase .',\n",
    "    ':transform_data_from_multi_select rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':transform_data_from_multi_select rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':transform_data_from_multi_select prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':transform_data_from_multi_select prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':transform_data_from_multi_select prov:qualifiedAssociation :{multi_select_uuid_writer} .',\n",
    "    f':{multi_select_uuid_writer} prov:agent :{multi_select_code_writer} .',\n",
    "    f':{multi_select_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{multi_select_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':transform_data_from_multi_select prov:used :data_after_transform_data_from_ordinal_encoding .',\n",
    "    ':data_after_transform_data_from_multi_select rdf:type prov:Entity .',\n",
    "    ':data_after_transform_data_from_multi_select rdfs:label \"dataset after multi select transformation\" .',\n",
    "    ':data_after_transform_data_from_multi_select prov:wasGeneratedBy :transform_data_from_multi_select .',\n",
    "    ':data_after_transform_data_from_multi_select prov:wasDerivedFrom :data_after_transform_data_from_ordinal_encoding .',\n",
    "]\n",
    "engine.insert(multi_select_activity, prefixes=prefixes)\n",
    "print(\"Inserted writer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printColumns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    print(df.columns.tolist())\n",
    "    print(len(df.columns))\n",
    "    #colsToShow = [\"LanguageHaveWorkedWith\", \"Dart\", \"Java\", \"Kotlin\", \"Python\", \"SQL\", \"Delphi\", \"HTML/CSS\", \"Bash/Shell (all shells)\",] \n",
    "    #colsToShow = [\"PlatformHaveWorkedWith\", \"Cloud Firestore\", \"PostgreSQL\", \"MongoDB\", \"DuckDB\"]\n",
    "    #colsToShow = [\"PlatformHaveWorkedWith\", \"Amazon Web Services (AWS)\", \"Cloudflare\", \"Google Cloud\", \"Docker\", \"Firebase\", \"npm\", \"pnpm\"]\n",
    "    #colsToShow = [\"WebframeHaveWorkedWith\", \"Spring Boot\", \"Next.js\", \"Node.js\", \"Angular\", \"ASP.NET\"]\n",
    "    colsToShow = [\"DevEnvsHaveWorkedWith\", \"Android Studio\", \"Notepad++\", \"Visual Studio\", \"Visual Studio Code\", \"Xcode\", \"Eclipse\"]\n",
    "    #colsToShow = [\"OpSysProfessional use\", \"Windows\", \"Ubuntu\", \"MacOS\", \"Windows Subsystem for Linux (WSL)\"]\n",
    "    #colsToShow = [\"EmploymentAddl\", \"Attending school (full-time)\", \"Attending school (part-time)\", \"Engaged in paid work (less than 10 hours per week)\", \"Engaged in paid work (10-19 hours per week)\", \"Engaged in paid work (20-29 hours per week)\", \"Transitioning to retirement (gradually reducing work hours)\", \"Volunteering (regularly)\", \"Caring for dependents (children, elderly, etc.)\", \"None of the above\"]\n",
    "    #colsToShow = [\"RemoteWork\"]\n",
    "    print(df[colsToShow].head(15))\n",
    "\n",
    "\n",
    "printColumns(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3f9e5",
   "metadata": {},
   "source": [
    "#### 6 Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MainBranch = [\"I am a developer by profession\", \"I am not primarily a developer, but I write code sometimes as part of my work/studies\", \"I used to be a developer by profession, but no longer am\", \"I am learning to code\", \"I code primarily as a hobby\", \"I work with developers or my work supports developers but am not a developer by profession\", \"None of these\"]\n",
    "employmentStatus = [\"Employed\", \"Independent contractor, freelancer, or self-employed\", \"Not employed\", \"Student\", \"Retired\", \"I prefer not to say\"]\n",
    "devTypes = [\"AI/ML engineer\", \"Architect, software or solutions\", \"Cloud infrastructure engineer\", \"Cybersecurity or InfoSec professional\", \"Data engineer\", \"Data or business analyst\", \"Data scientist\", \"Database administrator or engineer\", \"Developer, AI apps or physical AI\", \"Developer, back-end\", \"Developer, desktop or enterprise applications\", \"Developer, embedded applications or devices\", \"Developer, front-end\", \"Developer, full-stack\", \"Developer, game or graphics\", \"Developer, mobile\", \"Developer, QA or test\", \"DevOps engineer or professional\", \"Founder, technology or otherwise\", \"Product manager\", \"Project manager\", \"Senior executive (C-suite, VP, etc.)\", \"Student\", \"Support engineer or analyst\", \"System administrator\", \"UX, Research Ops or UI design professional\"]\n",
    "#The following DevTypes have been excluded: Academic researcher, Applied scientist, Engineering manager, Financial analyst or engineer, Retired\n",
    "devTypesToDelete = [\"Academic researcher\", \"Applied scientist\", \"Engineering manager\", \"Financial analyst or engineer\", \"Retired\"]\n",
    "industries = [\"Software Development\", \"Computer Systems Design and Services\", \"Internet, Telecomm or Information Services\", \"Fintech\", \"Energy\", \"Government\", \"Banking/Financial Services\", \"Manufacturing\", \"Transportation, or Supply Chain\", \"Healthcare\", \"Retail and Consumer Services\", \"Higher Education\", \"Media & Advertising Services\", \"Insurance\", \"Other\"]\n",
    "countries = [\"United States of America\", \"Germany\", \"India\", \"United Kingdom of Great Britain and Northern Ireland\", \"France\", \"Canada\", \"Ukraine\", \"Poland\", \"Netherlands\", \"Italy\", \"Brazil\", \"Australia\", \"Spain\", \"Sweden\", \"Switzerland\", \"Czech Republic\", \"Austria\", \"Romania\", \"Belgium\", \"Other\"]\n",
    "remoteWork = [\"Remote\", \"In-person\", \"Hybrid (some remote, leans heavy to in-person)\", \"Hybrid (some in-person, leans heavy to flexibility)\", \"Your choice (very flexible, you can come in when you want or just as needed)\"]\n",
    "\n",
    "def addColumns(df: pd.DataFrame,) -> pd.DataFrame:\n",
    "    allColumns = MainBranch + employmentStatus + devTypes + industries + countries + remoteWork\n",
    "\n",
    "    for column in allColumns:\n",
    "        if column not in df.columns:\n",
    "            df[column] = 0\n",
    "    return df\n",
    "\n",
    "processed_data = addColumns(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c59827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for deleting all unwatned rows regarding DevType\n",
    "def deleteUnwantedDevtypes(df, source_col=\"DevType\"):\n",
    "    cleaned = df[source_col].fillna(\"\").astype(str).str.split(\";\")\n",
    "    \n",
    "    mask_unwanted = cleaned.apply(\n",
    "        lambda roles: any(item.strip() in devTypesToDelete for item in roles)\n",
    "    )\n",
    "    \n",
    "    df = df[~mask_unwanted].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "def mainBranchToColumn(df: pd.DataFrame, source_col: str = \"MainBranch\") -> pd.DataFrame:\n",
    "    unique_vals = df[source_col].dropna().unique()\n",
    "\n",
    "    for val in unique_vals:\n",
    "        df[val] = (df[source_col] == val).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def devTypesToColumn(df: pd.DataFrame, source_col: str = \"DevType\") -> pd.DataFrame:\n",
    "    unique_vals = df[source_col].dropna().unique()\n",
    "\n",
    "    for val in unique_vals:\n",
    "        df[val] = (df[source_col] == val).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def employmentStatusToColumn(df: pd.DataFrame, source_col: str = \"Employment\") -> pd.DataFrame:\n",
    "    unique_vals = df[source_col].dropna().unique()\n",
    "\n",
    "    for val in unique_vals:\n",
    "        df[val] = (df[source_col] == val).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def industriesToColumn(df: pd.DataFrame, source_col: str = \"Industry\") -> pd.DataFrame:\n",
    "    unique_vals = df[source_col].dropna().unique()\n",
    "\n",
    "    for val in unique_vals:\n",
    "        df[val] = (df[source_col] == val).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def countriesToColumn(df: pd.DataFrame, source_col: str = \"Country\") -> pd.DataFrame:\n",
    "    for val in countries:\n",
    "        df[val] = (df[source_col] == val).astype(int)\n",
    "\n",
    "    df[\"Other\"] = df[source_col].apply(lambda x: 0 if x in countries else 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def remoteWorkToColumn(df: pd.DataFrame, source_col: str = \"RemoteWork\") -> pd.DataFrame:\n",
    "    unique_vals = df[source_col].dropna().unique()\n",
    "\n",
    "    for val in unique_vals:\n",
    "        df[val] = (df[source_col] == val).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def processAllCategoricalData(df: pd.DataFrame,  outliers_report: dict) -> pd.DataFrame:\n",
    "    processed_data_temp = mainBranchToColumn(processed_data)\n",
    "    processed_data_temp = employmentStatusToColumn(processed_data_temp)\n",
    "    processed_data_temp = devTypesToColumn(processed_data_temp)\n",
    "    processed_data_temp = industriesToColumn(processed_data_temp)\n",
    "    processed_data_temp = countriesToColumn(processed_data_temp)\n",
    "    processed_data_temp = remoteWorkToColumn(processed_data_temp)\n",
    "\n",
    "    return processed_data_temp\n",
    "\n",
    "start_time_td = now()\n",
    "processed_data = processAllCategoricalData(processed_data, outliers_report)\n",
    "processed_data = deleteUnwantedDevtypes(processed_data)\n",
    "end_time_td = now()\n",
    "\n",
    "categorical_code_writer = student_b\n",
    "\n",
    "categorical_uuid_executor = \"574dc9ed-a79a-4106-9f49-319be6f0b022\"\n",
    "categorical_executor = [\n",
    "    f':transform_data_from_categorical prov:qualifiedAssociation :{categorical_uuid_executor} .',\n",
    "    f':{categorical_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{categorical_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{categorical_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(categorical_executor, prefixes=prefixes)\n",
    "\n",
    "print(\"Inserted executor\")\n",
    "\n",
    "categorical_uuid_writer = \"2ccf0faa-0bda-404b-b14d-84fbb9658dff\"\n",
    "td_comment = \"\"\"\n",
    "Similar to the last two the columns with categorical data were also transformed in order to have a column for every single possible value. Afterwads we again applied binary encoded. For non existent values like NaN we again gave a 0 in every column. \n",
    "We further deleted the rows with DevTypes, that we do not want in our model.\n",
    "\"\"\"\n",
    "\n",
    "categorical_activity = [\n",
    "    ':transform_data_from_categorical rdf:type prov:Activity .',\n",
    "    ':transform_data_from_categorical rdfs:label \"Transform categorical data\" .',\n",
    "    ':transform_data_from_categorical sc:isPartOf :data_preparation_phase .',\n",
    "    ':transform_data_from_categorical rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':transform_data_from_categorical rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':transform_data_from_categorical prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':transform_data_from_categorical prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':transform_data_from_categorical prov:qualifiedAssociation :{categorical_uuid_writer} .',\n",
    "    f':{categorical_uuid_writer} prov:agent :{categorical_code_writer} .',\n",
    "    f':{categorical_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{categorical_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':transform_data_from_categorical prov:used :data_after_transform_data_from_multi_select .',\n",
    "    ':data_after_transform_data_from_categorical rdf:type prov:Entity .',\n",
    "    ':data_after_transform_data_from_categorical rdfs:label \"dataset after categorical transformation\" .',\n",
    "    ':data_after_transform_data_from_categorical prov:wasGeneratedBy :transform_data_from_categorical .',\n",
    "    ':data_after_transform_data_from_categorical prov:wasDerivedFrom :data_after_transform_data_from_multi_select .',\n",
    "]\n",
    "engine.insert(categorical_activity, prefixes=prefixes)\n",
    "print(\"Inserted writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "\n",
    "\n",
    "def printColumns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    print(df.columns.tolist())\n",
    "    print(len(df.columns))\n",
    "    #colsToShow = [\"MainBranch\", \"I am a developer by profession\", \"I am not primarily a developer, but I write code sometimes as part of my work/studies\", \"I used to be a developer by profession, but no longer am\", \"I am learning to code\", \"I code primarily as a hobby\", \"I work with developers or my work supports developers but am not a developer by profession\", \"None of these\"]\n",
    "    #colsToShow = [\"Employment\", \"Employed\", \"Independent contractor, freelancer, or self-employed\", \"Not employed\", \"Student\", \"Retired\", \"I prefer not to say\"]\n",
    "    #colsToShow = [\"DevType\", \"AI/ML engineer\", \"Architect, software or solutions\", \"Cloud infrastructure engineer\", \"Cybersecurity or InfoSec professional\", \"Data engineer\", \"Data or business analyst\", \"Data scientist\", \"Database administrator or engineer\", \"Developer, AI apps or physical AI\", \"Developer, back-end\", \"Developer, desktop or enterprise applications\", \"Developer, embedded applications or devices\", \"Developer, front-end\", \"Developer, full-stack\", \"Developer, game or graphics\", \"Developer, mobile\", \"Developer, QA or test\", \"DevOps engineer or professional\", \"Founder, technology or otherwise\", \"Product manager\", \"Project manager\", \"Senior executive (C-suite, VP, etc.)\", \"Student\", \"Support engineer or analyst\", \"System administrator\", \"UX, Research Ops or UI design professional\"]\n",
    "    #colsToShow = [\"Industry\", \"Software Development\", \"Computer Systems Design and Services\", \"Internet, Telecomm or Information Services\", \"Fintech\", \"Energy\", \"Government\", \"Banking/Financial Services\", \"Manufacturing\", \"Transportation, or Supply Chain\", \"Healthcare\", \"Retail and Consumer Services\", \"Higher Education\", \"Media & Advertising Services\", \"Insurance\", \"Other\"]\n",
    "    #colsToShow = [\"Country\", \"United States of America\", \"Germany\", \"India\", \"United Kingdom of Great Britain and Northern Ireland\", \"France\", \"Canada\", \"Ukraine\", \"Poland\", \"Netherlands\", \"Italy\", \"Brazil\", \"Australia\", \"Spain\", \"Sweden\", \"Switzerland\", \"Czech Republic\", \"Austria\", \"Romania\", \"Belgium\", \"Other\"]\n",
    "    colsToShow = [\"RemoteWork\", \"Remote\", \"In-person\", \"Hybrid (some remote, leans heavy to in-person)\", \"Hybrid (some in-person, leans heavy to flexibility)\", \"Your choice (very flexible, you can come in when you want or just as needed)\"]\n",
    "    print(df[colsToShow].head())\n",
    "\n",
    "\n",
    "printColumns(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5e5e2",
   "metadata": {},
   "source": [
    "#### 7 Delete not needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bfc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropSelectedColumns(df: pd.DataFrame,  outliers_report: dict) -> pd.DataFrame:\n",
    "    columns_to_drop = [\n",
    "        \"Age\",\n",
    "        \"EdLevel\",\n",
    "        \"OrgSize\",\n",
    "        \"AISelect\",\n",
    "        \"AISent\",\n",
    "        \"LanguageHaveWorkedWith\",\n",
    "        \"DatabaseHaveWorkedWith\",\n",
    "        \"PlatformHaveWorkedWith\",\n",
    "        \"WebframeHaveWorkedWith\",\n",
    "        \"DevEnvsHaveWorkedWith\",\n",
    "        \"OpSysProfessional use\",\n",
    "        \"EmploymentAddl\",\n",
    "        \"MainBranch\",\n",
    "        \"DevType\",\n",
    "        \"Employment\",\n",
    "        \"Industry\",\n",
    "        \"Country\",\n",
    "        \"RemoteWork\",\n",
    "        \"DevType\",\n",
    "        \"JobSatPoints_15_TEXT\",\n",
    "        \"LanguagesHaveEntry\",\n",
    "        \"LanguagesWantEntry\",\n",
    "        \"DatabaseHaveEntry\",\n",
    "        \"DatabaseWantEntry\",\n",
    "        \"PlatformHaveEntry\",\n",
    "        \"PlatformWantEntry\",\n",
    "        \"WebframeHaveEntry\",\n",
    "        \"WebframeWantEntry\",\n",
    "        \"DevEnvHaveEntry\",\n",
    "        \"DevEnvWantEntry\",\n",
    "        \"SOTagsHaveEntry\",\n",
    "        \"SOTagsWant Entry\",\n",
    "        \"OfficeStackHaveEntry\",\n",
    "        \"OfficeStackWantEntry\",\n",
    "        \"CommPlatformHaveEntr\",\n",
    "        \"CommPlatformWantEntr\",\n",
    "        \"AIModelsHaveEntry\",\n",
    "        \"AIModelsWantEntry\",\n",
    "        \"SO_Actions_15_TEXT\",\n",
    "        \"AIAgentKnowWrite\",\n",
    "        \"AIAgentOrchWrite\",\n",
    "        \"AIAgentObsWrite\",\n",
    "        \"AIAgentExtWrite\",\n",
    "        \"AIOpen\",\n",
    "        \"TechOppose_15_TEXT\",\n",
    "        \"TechEndorse_13_TEXT\",\n",
    "    ]\n",
    "\n",
    "    existing_cols = [c for c in columns_to_drop if c in df.columns]\n",
    "\n",
    "    return df.drop(columns=existing_cols, errors=\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_time_td = now()\n",
    "processed_data = dropSelectedColumns(processed_data, outliers_report)\n",
    "end_time_td = now()\n",
    "\n",
    "remove_not_needed_columns_code_writer = student_b\n",
    "\n",
    "remove_not_needed_columns_uuid_executor = \"a799a635-5e7c-48bd-8bfb-567de7a262f1\"\n",
    "remove_not_needed_columns_executor = [\n",
    "    f':remove_not_needed_columns prov:qualifiedAssociation :{remove_not_needed_columns_uuid_executor} .',\n",
    "    f':{remove_not_needed_columns_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{remove_not_needed_columns_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{remove_not_needed_columns_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(remove_not_needed_columns_executor, prefixes=prefixes)\n",
    "\n",
    "print(\"Inserted executor\")\n",
    "\n",
    "remove_not_needed_columns_uuid_writer = \"4873b5a0-962f-481e-ad81-d354b402767c\"\n",
    "td_comment = \"\"\"\n",
    "In this part we removed the rest of all columns that are not needed within our model.\n",
    "\"\"\"\n",
    "\n",
    "remove_not_needed_columns_activity = [\n",
    "    ':remove_not_needed_columns rdf:type prov:Activity .',\n",
    "    ':remove_not_needed_columns rdfs:label \"Remove not needed columns\" .',\n",
    "    ':remove_not_needed_columns sc:isPartOf :data_preparation_phase .',\n",
    "    ':remove_not_needed_columns rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':remove_not_needed_columns rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':remove_not_needed_columns prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':remove_not_needed_columns prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':remove_not_needed_columns prov:qualifiedAssociation :{remove_not_needed_columns_uuid_writer} .',\n",
    "    f':{remove_not_needed_columns_uuid_writer} prov:agent :{remove_not_needed_columns_code_writer} .',\n",
    "    f':{remove_not_needed_columns_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{remove_not_needed_columns_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':remove_not_needed_columns prov:used :data_after_transform_data_from_categorical .',\n",
    "    ':data_after_remove_not_needed_columns rdf:type prov:Entity .',\n",
    "    ':data_after_remove_not_needed_columns rdfs:label \"dataset after removal of not needed columns\" .',\n",
    "    ':data_after_remove_not_needed_columns prov:wasGeneratedBy :remove_not_needed_columns .',\n",
    "    ':data_after_remove_not_needed_columns prov:wasDerivedFrom :data_after_transform_data_from_categorical .',\n",
    "]\n",
    "engine.insert(remove_not_needed_columns_activity, prefixes=prefixes)\n",
    "print(\"Inserted writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = os.path.join(\"data\", \"prepared\")\n",
    "output_file = os.path.join(output_dir, \"final_dataset.csv\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "processed_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0036428-fcdf-4ee8-ad52-424f95024cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your final transformed dataset should also be documented appropriately using Croissant, SI, etc.\n",
    "\n",
    "prepared_data_triples = [\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    ':prepared_data prov:wasDerivedFrom :cleaned_data .',\n",
    "    ':prepared_data rdf:type sc:Dataset .',\n",
    "    # ....\n",
    "]\n",
    "engine.insert(prepared_data_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b5a04",
   "metadata": {},
   "source": [
    "### Describing pre-processing steps considered but not applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746c3d7",
   "metadata": {},
   "source": [
    "**Removing Outliers**<br>\n",
    "Regarding the removal of the outliers we had to separate approaches in order to handle them, one being the deletion of every row that has a salary above 500.000 per year and the second one being the deletion with the usage of perceptiles.<br>\n",
    "In the end we decided against taking absolute number as a border for deletion, because it is in our opinion not wise to do that. Having absolut numbers as a salary cap me be working for now, but in the future when salaries generally rise, the border of 500.000 might be outdated and therefore could lead us to wrong decisions. With using perceptiles on the other hand, you always go with the relative flow of the numbers.\n",
    "\n",
    "**Binning**<br>\n",
    "We did not use binning at all (apart of the already \"binned\" columns like age) because with binning for the salaries of the developers we would have created a classification out of our regression problem, which wouldnt be advisable at all. So there haven't been any pre-processing steps been considered at all.\n",
    "\n",
    "**Attribute removal**<br>\n",
    "Regarding attribute removal we had a long discussion about which attributes to use and which to eliminate from our dataset, always debating whether these attributes would have contributed something to our problem / model or not. Especially regarding the attribute \"RemoteWork\", which basically listed all types of working (from fully remote to fully in the office) we had a long discussion about whether this attribute would be interesting or not. After a bit of searching through the internet we then decided to keep the attribute and therefore did not apply the step of attribute removal, as there may be an interesting correlation between remotely working and the corresponding salary.\n",
    "\n",
    "**ONe-Hot Encoding for countries**<br>\n",
    "We decided against doing one-Hot encoding for every country as it would have resulted in more than 150 columns to cover each country. We therefore decided to analyze how many countries developers are withing the Top 5, 10, 15, and 20 and then decided to go with Top-N-Encoding.\n",
    "\n",
    "**Textual fields**<br>\n",
    "We considered to go through and analyze all columns which consisted of testual answers that were given by the developers, but decided against it, as it would have been far to much to analyze and also difficult to properly encode it with the right values. Therefore we decided to drop all these attributesin order to also guarantee that we have less noisy features within our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "peprocessing_steps_not_applied_code_writer = student_b\n",
    "\n",
    "preprocessing_steps_not_applied_uuid_executor = \"55340241-cb18-4f1e-a6ae-651fb9b141ba\"\n",
    "preprocessing_steps_not_applied_executor = [\n",
    "    f':preprocessing_steps_considered_but_not_applied prov:qualifiedAssociation :{preprocessing_steps_not_applied_uuid_executor} .',\n",
    "    f':{preprocessing_steps_not_applied_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{preprocessing_steps_not_applied_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{preprocessing_steps_not_applied_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(preprocessing_steps_not_applied_executor, prefixes=prefixes)\n",
    "print(\"Inserted executor\")\n",
    "\n",
    "td_comment = \"\"\"Removing Outliers*\n",
    "Regarding the removal of the outliers we had to separate approaches in order to handle them, one being the removal of every row that has a salary above 500.000 per year and the second one being the removal with the usage of perceptiles.<br>\n",
    "In the end we decided against taking absolute number as a border for removal, because it is in our opinion not wise to do that. Having absolut numbers as a salary cap me be working for now, but in the future when salaries generally rise, the border of 500.000 might \n",
    "be outdated and therefore could lead us to wrong decisions. With using perceptiles on the other hand, you always go with the relative flow of the numbers.\n",
    "\n",
    "Binning\n",
    "We did not use binning at all (apart of the already \"binned\" columns like age) because with binning for the salaries of the developers we would have created a classification out of our regression problem, which wouldnt be advisable at all.\n",
    "So there haven't been any pre-processing steps been considered at all.\n",
    "\n",
    "Attribute removal\n",
    "Regarding attribute removal we had a long discussion about which attributes to use and which to remove from our dataset, always debating whether these attributes would have contributed something to our problem / model or not. Especially regarding the attribute \"RemoteWork\",\n",
    "which basically listed all types of working (from fully remote to fully in the office) we had a long discussion about whether this attribute would be interesting or not. \n",
    "After a bit of searching through the internet we then decided to keep the attribute and therefore did not apply the step of attribute removal, as there may be an interesting correlation between remotely working and the corresponding salary.\n",
    "\n",
    "ONe-Hot Encoding for countries\n",
    "We decided against doing one-Hot encoding for every country as it would have resulted in more than 150 columns to cover each country. We therefore decided to analyze how many countries developers are withing the Top 5, 10, 15, and 20 and then decided to go with Top-N-Encoding.\n",
    "\n",
    "Textual fields\n",
    "We considered to go through and analyze all columns which consisted of testual answers that were given by the developers, but decided against it, as it would have been far to much to analyze and also difficult to properly encode it with the right values.\n",
    "Therefore we decided to remove all these attributesin order to also guarantee that we have less noisy features within our dataset.\"\"\"\n",
    "\n",
    "preprocessing_steps_not_applied_uuid_writer   = \"b11735ff-bc57-4add-a02f-c7cec19774e3\"\n",
    "preprocessing_steps_not_applied_activity = [\n",
    "    ':preprocessing_steps_considered_but_not_applied rdf:type prov:Activity .',\n",
    "    ':preprocessing_steps_considered_but_not_applied rdfs:label \"Preprocessing steps that have been considered but not applied\" .',\n",
    "    ':preprocessing_steps_considered_but_not_applied sc:isPartOf :data_preparation_phase .',\n",
    "    ':preprocessing_steps_considered_but_not_applied rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':preprocessing_steps_considered_but_not_applied rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':preprocessing_steps_considered_but_not_applied prov:qualifiedAssociation :{preprocessing_steps_not_applied_uuid_writer} .',\n",
    "    f':{preprocessing_steps_not_applied_uuid_writer} prov:agent :{peprocessing_steps_not_applied_code_writer} .',\n",
    "    f':{preprocessing_steps_not_applied_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{preprocessing_steps_not_applied_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    #Everything else below in my opinion not needed, as there is no Output after running these steps.\n",
    "]\n",
    "engine.insert(preprocessing_steps_not_applied_activity, prefixes=prefixes)\n",
    "print(\"Inserted writer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d805d",
   "metadata": {},
   "source": [
    "### Derived attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7c873",
   "metadata": {},
   "source": [
    "**Countries --> Regions**<br>\n",
    "One possible option for making a derived attributes would have been to group countries to regions / continents. But as there are a lot of differences betweend for example western and eastern europe, we decided against it.\n",
    "\n",
    "**Work experience in years --> seniority level**<br>\n",
    "We could have created a column and classify developers based on their working experience as junioer, senior, etc. But as the Working experience is a numerical input anyway and also is \"better\" the higher the value is, it didn't make much sense for us. We further think, that with kind of grouping developers based on their work experience, we might create a bias which we do not want.\n",
    "\n",
    "**Job satisfactory**<br>\n",
    "Regarding the job satisfactory we also could have created derived attributes that for example say if you gave your job between 10-14 points jour job is satisfying, but with our regression problem this wouldnt have made any sense as well.\n",
    "\n",
    "**1-to-n coding**<br>\n",
    "The only real thing we did regarding the deriving of attributes was to make use of the 1-to-n encoding. As we had a lot of attributes that were either ordinal, Multi-Select or categorical, this was in our opinion the best way in order to transform the values of the affected attributes to make our dataset ready for the modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f21177",
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_attributes_code_writer = student_b\n",
    "\n",
    "derived_attributes_uuid_executor = \"39b3d2a1-d866-4342-bc15-b02a2090a20e\"\n",
    "derived_attributes_executor = [\n",
    "    f':derived_attributes prov:qualifiedAssociation :{derived_attributes_uuid_executor} .',\n",
    "    f':{derived_attributes_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{derived_attributes_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{derived_attributes_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(derived_attributes_executor, prefixes=prefixes)\n",
    "print(\"Inserted executor\")\n",
    "\n",
    "td_comment = \"\"\"\n",
    "Countries --> Regions\n",
    "One possible option for making a derived attributes would have been to group countries to regions / continents. But as there are a lot of differences betweend for example western and eastern europe, we decided against it.\n",
    "\n",
    "Work experience in years --> seniority level\n",
    "We could have created a column and classify developers based on their working experience as junioer, senior, etc. But as the Working experience is a numerical input anyway and also is \"better\" the higher the value is,\n",
    "it didn't make much sense for us. We further think, that with kind of grouping developers based on their work experience, we might create a bias which we do not want.\n",
    "\n",
    "Job satisfactory\n",
    "Regarding the job satisfactory we also could have created derived attributes that for example say if you gave your job between 10-14 points jour job is satisfying, but with our regression problem this wouldnt have made any sense as well.\n",
    "\n",
    "1-to-n coding\n",
    "The only real thing we did regarding the deriving of attributes was to make use of the 1-to-n encoding. As we had a lot of attributes that were either ordinal, Multi-Select or categorical, this was in our opinion the best way in order\n",
    "to transform the values of the affected attributes to make our dataset ready for the modelling..\"\"\"\n",
    "\n",
    "derived_attribute_uuid_writer   = \"8f776548-9fd7-4b74-ae98-ada840cd6b8e\"\n",
    "derived_attributes_activity = [\n",
    "    ':derived_attributes rdf:type prov:Activity .',\n",
    "    ':derived_attributes rdfs:label \"Derived attributes\" .',\n",
    "    ':derived_attributes sc:isPartOf :data_preparation_phase .',\n",
    "    ':derived_attributes rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':derived_attributes rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':derived_attributes prov:qualifiedAssociation :{derived_attribute_uuid_writer} .',\n",
    "    f':{derived_attribute_uuid_writer} prov:agent :{derived_attributes_code_writer} .',\n",
    "    f':{derived_attribute_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{derived_attribute_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "]\n",
    "engine.insert(derived_attributes_activity, prefixes=prefixes)\n",
    "print(\"Inserted writer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234abfe",
   "metadata": {},
   "source": [
    "### External data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63799f92",
   "metadata": {},
   "source": [
    "**22700+ Software Professional Salary Dataset**<br>\n",
    "from Kaggle: https://www.kaggle.com/datasets/whenamancodes/software-professional-salary-dataset <br>\n",
    "This could have been a good alternative to our chosen dataset, but this one only has 14 attributes and in the specification it would have been at least 15, so we wouldnt have been able to use this dataset anyways. Furthermore, this dataset does not have any attributes, that our chosen dataset wouldnt have anyway.\n",
    "\n",
    "**Developer_Salary_By_Country**<br>\n",
    "from back4app: https://www.back4app.com/database/back4app/developer-salary-by-country/api <br>\n",
    "While doing our research we stumbled across this dataset right here, but in our opinion we wouldnt be able to derive some high quality model from this dataset. \n",
    "\n",
    "**Remote Software Developer Salaries Around the World**<br>\n",
    "from arc.dev: https://arc.dev/salaries <br>\n",
    "If we wanted to dive deeper into the salaries regarding developers that are working remote, this dataset would have actually been interesting for us. But like the other two datasets, we did not use them.\n",
    "\n",
    "**Best Salary Datasets & Databases**<br>\n",
    "from datarade.ai: https://datarade.ai/data-categories/salary-data/datasets <br>\n",
    "This datasets also look very good on the first sight, but we would have to be very careful on using one of these datasets, because some of them only cover one country like USA. So this probably would have created a BIAS, which we obviously do not want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d01afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_data_sources_code_writer = student_b\n",
    "\n",
    "external_data_sources_uuid_executor = \"c102e7bd-8089-491e-98ff-882a90cbb1fc\"\n",
    "external_data_sources_executor = [\n",
    "    f':external_data_sources prov:qualifiedAssociation :{external_data_sources_uuid_executor} .',\n",
    "    f':{external_data_sources_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{external_data_sources_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{external_data_sources_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(external_data_sources_executor, prefixes=prefixes)\n",
    "print(\"Inserted executor\")\n",
    "\n",
    "td_comment = \"\"\"\n",
    "22700+ Software Professional Salary Dataset\n",
    "from Kaggle: https://www.kaggle.com/datasets/whenamancodes/software-professional-salary-dataset\n",
    "This could have been a good alternative to our chosen dataset, but this one only has 14 attributes and in the specification it would have been at least 15, so we wouldnt have been able to use this dataset anyways. Furthermore, this dataset does not have any attributes, that our chosen dataset wouldnt have anyway.\n",
    "\n",
    "Developer_Salary_By_Country\n",
    "from back4app: https://www.back4app.com/database/back4app/developer-salary-by-country/api\n",
    "While doing our research we stumbled across this dataset right here, but in our opinion we wouldnt be able to derive some high quality model from this dataset. \n",
    "\n",
    "Remote Software Developer Salaries Around the World\n",
    "from arc.dev: https://arc.dev/salaries\n",
    "If we wanted to dive deeper into the salaries regarding developers that are working remote, this dataset would have actually been interesting for us. But like the other two datasets, we did not use them.\n",
    "\n",
    "Best Salary Datasets & Databases\n",
    "from datarade.ai: https://datarade.ai/data-categories/salary-data/datasets\n",
    "This datasets also look very good on the first sight, but we would have to be very careful on using one of these datasets, because some of them only cover one country like USA. So this probably would have created a BIAS, which we obviously do not want.\"\"\"\n",
    "\n",
    "external_data_sources_uuid_writer   = \"95e51af3-81ea-44fa-ad02-cba3a157506c\"\n",
    "external_data_sources_activity = [\n",
    "    ':external_data_sources rdf:type prov:Activity .',\n",
    "    ':external_data_sources rdfs:label \"External data sources\" .',\n",
    "    ':external_data_sources sc:isPartOf :data_preparation_phase .',\n",
    "    ':external_data_sources rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':external_data_sources rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':external_data_sources prov:qualifiedAssociation :{external_data_sources_uuid_writer} .',\n",
    "    f':{derived_attribute_uuid_writer} prov:agent :{external_data_sources_code_writer} .',\n",
    "    f':{derived_attribute_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{derived_attribute_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "]\n",
    "engine.insert(external_data_sources_activity, prefixes=prefixes)\n",
    "print(\"Inserted writer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c19ebb",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb93dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "f':modeling_phase rdf:type prov:Activity .',\n",
    "f':modeling rdfs:label \"Modeling Phase\" .', \n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a\n",
    "#############################################\n",
    "\n",
    "dma_ass_uuid_writer = \"b3e840ab-ac23-415e-bd9c-6d00bb79c37a\"\n",
    "dma_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example algorithm definition\n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"Random Forest Algorithm\" .',\n",
    "\n",
    "    # example implementation\n",
    "    f':random_forrest_classifier_implementation rdf:type mls:Implementation .',\n",
    "    f':random_forrest_classifier_implementation rdfs:label \"Scikit-learn RandomForestClassifier\" .',\n",
    "    f':random_forrest_classifier_implementation mls:implements :random_forest_algorithm .',\n",
    "    f':random_forrest_classifier_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "    # you can also define your Evaluation Measures here\n",
    "    \n",
    "    # example evaluation \n",
    "    f':r2_score_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':r2_score_measure rdfs:label \"R-squared Score\" .',\n",
    "    f':r2_score_measure rdfs:comment \"xxx\" .',\n",
    "    f':r2_score_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation 4b\n",
    "#############################################\n",
    "\n",
    "hp_ass_uuid_writer = \"fff582a8-c5cd-4030-978b-9f56b603167c\"\n",
    "hp_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "identify_hp_activity = [\n",
    "    f':identify_hyperparameters rdf:type prov:Activity .',\n",
    "    f':identify_hyperparameters sc:isPartOf :modeling_phase .',\n",
    "    f':identify_hyperparameters rdfs:comment \"\"\"{hp_comment}\"\"\" .',\n",
    "    f':identify_hyperparameters prov:qualifiedAssociation :{hp_ass_uuid_writer} .',\n",
    "    f':{hp_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{hp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{hp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example parameter\n",
    "    f':hp_learning_rate rdf:type mls:HyperParameter .',\n",
    "    f':hp_learning_rate rdfs:label \"Learning Rate\" .',\n",
    "    f':hp_learning_rate rdfs:comment \"...\" .',\n",
    "    f':random_forrest_classifier_implementation mls:hasHyperParameter :hp_learning_rate .',\n",
    "    f':hp_learning_rate prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    # continue with your identified hyperparameters\n",
    "    \n",
    "]\n",
    "engine.insert(identify_hp_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df: pd.DataFrame):\n",
    "    #do something\n",
    "    return 'train_set', 'validation_set', 'test_set'\n",
    "\n",
    "#############################################\n",
    "# Documentation 4c\n",
    "#############################################\n",
    "\n",
    "### Define Train/Validation/Test splits\n",
    "split_ass_uuid_writer = \"fb58ae6c-9d58-44c9-ac7e-529111bdf7fc\"\n",
    "split_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "## Use your prepared dataset\n",
    "input_dataset = \":prepared_data\" \n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:comment \"Train/Validation/Test Split Definition\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "    \n",
    "    # Training Set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"Training Set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Validation Set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"Validation Set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Test Set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"Test Set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(define_split_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b5ed6-54d6-4c81-9adb-e295fbd5c364",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-978b274ef875c238",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_finetune_model(training_set, validation_set):\n",
    "    # do something here\n",
    "\n",
    "    # Try to automate as much documentation work as possible.\n",
    "    # Define your training runs with their respective hyperparameter settings, etc.\n",
    "    # Document each time a training run, model, its hp_settings, evaluations, ...  \n",
    "    # Create performance figures/graphs\n",
    "\n",
    "    return 'Find most suitable model'\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4d & e & f\n",
    "#############################################\n",
    "\n",
    "tafm_ass_uuid_writer = \"21d60fe3-c9ab-4a0a-bae7-b9fe9653c755\"\n",
    "tafm_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# EXAMPLE output from your training\n",
    "training_run1 = \"run_1\" \n",
    "model_run1 = \"model_run1\"\n",
    "hp1_setting_run1 = \"hp_setting_run1\"\n",
    "eval_train_run1 = \"metric_train_run1\"\n",
    "eval_validation_run1 = \"metric_validation_run1\"\n",
    "\n",
    "\n",
    "train_model_activity = [\n",
    "    # Activity \n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    ########################################\n",
    "    # ONE model run - automate everything below!\n",
    "\n",
    "    # Parameter settings\n",
    "    f':{hp1_setting_run1} rdf:type mls:HyperParameterSetting .',\n",
    "    f':{hp1_setting_run1} mls:specifiedBy :hp_learning_rate .',\n",
    "    f':{hp1_setting_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{hp1_setting_run1} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "    # add your further parameters\n",
    "\n",
    "    # Describe your Run\n",
    "    f':{training_run1} rdf:type mls:Run .',\n",
    "    f':{training_run1} sc:isPartOf :train_and_finetune_model .',\n",
    "    f':{training_run1} mls:realizes :random_forest_algorithm .',\n",
    "    f':{training_run1} rdf:label \"Training Run 1 with...\" .',\n",
    "    f':{training_run1} mls:executes :your_implementation .', \n",
    "    f':{training_run1} mls:hasInput :training_set .',\n",
    "    f':{training_run1} mls:hasInput :validation_set .',\n",
    "    f':{training_run1} mls:hasInput :{hp1_setting_run1} .',     \n",
    "    # list all your used parameters here\n",
    "    f':{training_run1} mls:hasOutput :{model_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_train_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_validation_run1} .',\n",
    "\n",
    "    # Describe your Model\n",
    "    f':{model_run1} rdf:type mls:Model .',\n",
    "    f':{model_run1} prov:label \"xxx\" .',\n",
    "    f':{model_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{model_run1} mlso:trainedOn :training_set .',\n",
    "    f':{model_run1} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "\n",
    "    # Describe your evaluations\n",
    "    # You can have multiple evaluations per model \n",
    "    f':{eval_train_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_train_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_train_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_train_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_train_run1} prov:used :training_set .',\n",
    "\n",
    "    f':{eval_validation_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_validation_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_validation_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_validation_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_validation_run1} prov:used :validation_set .',\n",
    "\n",
    "    # Dont forget to document any visualizations\n",
    "\n",
    "]\n",
    "engine.insert(train_model_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model_full_data(training_set, validation_set):\n",
    "    \n",
    "    # create your\n",
    "    return \"Final Trained Model\"\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4g\n",
    "#############################################\n",
    "\n",
    "retrain_ass_uuid_writer = \"96815ee0-524c-437b-b5fa-2e15b945c993\" # Generate once\n",
    "\n",
    "final_training_activity = \":retrain_final_model\"\n",
    "final_model = \":final_model_entity\"\n",
    "\n",
    "# Document the retraining activity.\n",
    "# Hint: This activity is still part of the :modeling_phase\n",
    "\n",
    "retrain_documentation = [\n",
    "    # your documentation here    \n",
    "]\n",
    "engine.insert(retrain_documentation, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02059271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06583f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88bf71f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46137067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "f':evaluation_phase rdf:type prov:Activity .',\n",
    "f':evaluation_phase rdfs:label \"Evaluation Phase\" .', \n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d80e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_code_writer = student_b\n",
    "def evaluate_on_test_data(final_model, test_set):\n",
    "\n",
    "    # Predict and evaluation on test data\n",
    "        \n",
    "    return 'Performance'\n",
    "\n",
    "start_time_eval = now()\n",
    "#evaluate_on_test_data()\n",
    "end_time_eval = now() \n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "eval_ass_uuid = \"7f1431e9-feed-429a-92ed-c131b23cbe79\" # Generate once\n",
    "final_model = \":final_model_entity\" \n",
    "test_set = \":test_set\" \n",
    "\n",
    "eval_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    f':evaluate_final_model rdf:type prov:Activity .',\n",
    "    f':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_final_model rdfs:label \"Final Model Evaluation on Test Set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "    \n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Inputs\n",
    "    f':evaluate_final_model prov:used {final_model} .',\n",
    "    f':evaluate_final_model prov:used {test_set} .',\n",
    "    \n",
    "    # Reference to Data Mining Success Criteria from Phase 1\n",
    "    f':evaluate_final_model prov:used :bu_data_mining_success_criteria .',\n",
    "\n",
    "    # Document you final model performance\n",
    " \n",
    "    # Hint: you evaluate bias in this way:\n",
    "    f':bias_evaluation_result rdf:type mls:ModelEvaluation .',\n",
    "    f':bias_evaluation_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':bias_evaluation_result rdfs:label \"Bias Analysis\" .',\n",
    "    f':bias_evaluation_result rdfs:comment \"...\" .',\n",
    "    \n",
    "]\n",
    "engine.insert(evaluate_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785c94b",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "f':deployment_phase rdf:type prov:Activity .',\n",
    "f':deployment_phase rdfs:label \"Deployment Phase\" .', \n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "ethical_aspects_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "monitoring_plan_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "dep_ass_uuid_executor = \"72a921e0-1234-4567-89ab-cdef01234567\" # Generate once\n",
    "deployment_executor = [\n",
    "f':plan_deployment rdf:type prov:Activity .',\n",
    "f':plan_deployment sc:isPartOf :deployment_phase .', # Connect to Parent Phase\n",
    "f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .', \n",
    "]\n",
    "engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "deployment_data_executor = [\n",
    "#6a\n",
    "f':dep_recommendations rdf:type prov:Entity .',\n",
    "f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "#6b\n",
    "f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "#6c\n",
    "f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "#6d\n",
    "f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(deployment_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528dac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d410af",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f44e16",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"), \n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"), \n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes exemplary queries for different phases\n",
    "\n",
    "\n",
    "### Author Block\n",
    "author_query = f\"\"\"\n",
    "{prefix_header}\n",
    "PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT DISTINCT ?uri ?given ?family ?matr WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  \n",
    "  ?uri a foaf:Person .\n",
    "  ?uri foaf:givenName ?given .\n",
    "  ?uri foaf:familyName ?family .\n",
    "  ?uri iao:IAO_0000219 ?matr .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_authors = engine.query(author_query)\n",
    "author_block_latex = \"\"\n",
    "\n",
    "if not res_authors.empty: # type:ignore\n",
    "    for _, row in res_authors.iterrows(): # type:ignore\n",
    "\n",
    "        uri_str = str(row['uri'])\n",
    "        given = latex_escape(clean_rdf(row['given']))\n",
    "        family = latex_escape(clean_rdf(row['family']))\n",
    "        matr = latex_escape(clean_rdf(row['matr']))\n",
    "        if student_a in uri_str:\n",
    "            responsibility = \"Student A\"\n",
    "        elif student_b in uri_str:\n",
    "            responsibility = \"Student B\"\n",
    "        else:\n",
    "            responsibility = \"Student\"\n",
    "        \n",
    "        author_block_latex += rf\"\"\"\n",
    "          \\author{{{given} {family}}}\n",
    "          \\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "          \\affiliation{{\n",
    "            \\institution{{TU Wien}}\n",
    "            \\country{{Austria}}\n",
    "          }}\n",
    "          \"\"\"\n",
    "\n",
    "### Business Understanding example\n",
    "bu_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?ds_comment ?bo_comment WHERE {{\n",
    "  OPTIONAL {{ :bu_data_source_and_scenario rdfs:comment ?ds_comment . }}\n",
    "  OPTIONAL {{ :bu_business_objectives rdfs:comment ?bo_comment . }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_bu = engine.query(bu_query)\n",
    "row_bu = res_bu.iloc[0] if not res_bu.empty else {} # type:ignore\n",
    "bu_data_source = latex_escape(clean_rdf(row_bu.get(\"ds_comment\", \"\")))\n",
    "bu_objectives  = latex_escape(clean_rdf(row_bu.get(\"bo_comment\", \"\")))\n",
    "\n",
    "\n",
    "### Data Understanding examples\n",
    "# Example Dataset Description\n",
    "du_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?desc WHERE {{ :survey_recordset sc:description ?desc . }} LIMIT 1\n",
    "\"\"\"\n",
    "res_du_desc = engine.query(du_desc_query)\n",
    "row_du_desc = res_du_desc.iloc[0] if not res_du_desc.empty else {} # type:ignore\n",
    "du_description = latex_escape(clean_rdf(row_du_desc.get(\"desc\", \"\")))\n",
    "\n",
    "# Example Feature Columns Table\n",
    "\n",
    "# du_query = f\"\"\"\n",
    "# {prefix_header}\n",
    "\n",
    "# SELECT DISTINCT ?name ?dtypeRaw ?descRaw WHERE {{\n",
    "  # :survey_record_set cr:recordSet ?rs .\n",
    "  # ?rs cr:field ?field .\n",
    "  # ?field sc:name ?name .\n",
    "  # ?field sc:description ?descRaw .\n",
    "  # ?field cr:dataType ?dtypeRaw .\n",
    "# }}\n",
    "# \"\"\"\n",
    "# res_du = engine.query(du_query)\n",
    "du_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?name ?dtype ?desc WHERE {{\n",
    "  # Navigate from Dataset -> RecordSet -> Field\n",
    "  :loaded_survey_data cr:recordSet ?rs .\n",
    "  ?rs cr:field ?field .\n",
    "  \n",
    "  # Fetch the properties needed for your table\n",
    "  ?field sc:name ?name .\n",
    "  OPTIONAL {{ ?field cr:dataType ?dtype . }}\n",
    "  OPTIONAL {{ ?field sc:description ?desc . }}\n",
    "}}\n",
    "ORDER BY ?name\n",
    "\"\"\"\n",
    "\n",
    "res_du = engine.query(du_query)\n",
    "print(res_du)\n",
    "\n",
    "du_rows = []\n",
    "if not res_du.empty: # type:ignore\n",
    "    for _, f in res_du.iterrows(): # type:ignore\n",
    "        dtype_raw = clean_rdf(f.get(\"dtype\", \"\"))\n",
    "        if '#' in dtype_raw: dtype = dtype_raw.split('#')[-1]\n",
    "        elif '/' in dtype_raw: dtype = dtype_raw.split('/')[-1]\n",
    "        else: dtype = dtype_raw\n",
    "        \n",
    "        desc = clean_rdf(f.get(\"desc\", \"\"))\n",
    "        row_str = f\"{latex_escape(clean_rdf(f['name']))} & {latex_escape(dtype)} & {latex_escape(desc)} \\\\\\\\\"\n",
    "        du_rows.append(row_str)\n",
    "du_table_rows = \"\\n    \".join(du_rows)\n",
    "\n",
    "### Modeling example\n",
    "# Hyperparameters\n",
    "hp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?hpName (SAMPLE(?hpValRaw) as ?hpVal) (MAX(?hpDescRaw) as ?hpDesc) WHERE {{\n",
    "  ?run sc:isPartOf :train_and_finetune_model .\n",
    "  ?run mls:hasInput ?setting .\n",
    "  ?setting a mls:HyperParameterSetting .\n",
    "  ?setting mls:hasValue ?hpValRaw .\n",
    "  ?setting mls:specifiedBy ?hpDef .\n",
    "  ?hpDef rdfs:label ?hpName .\n",
    "  OPTIONAL {{ ?hpDef rdfs:comment ?hpDescRaw . }}\n",
    "}} \n",
    "GROUP BY ?hpName\n",
    "ORDER BY ?hpName\n",
    "\"\"\"\n",
    "res_hp = engine.query(hp_query)\n",
    "hp_rows = []\n",
    "if not res_hp.empty: #type:ignore\n",
    "    for _, row in res_hp.iterrows(): #type:ignore\n",
    "        name = latex_escape(clean_rdf(row['hpName']))\n",
    "        val  = latex_escape(clean_rdf(row['hpVal']))\n",
    "        desc = latex_escape(clean_rdf(row.get('hpDesc', '')))\n",
    "        hp_rows.append(rf\"{name} & {desc} & {val} \\\\\")\n",
    "\n",
    "hp_table_rows = \"\\n    \".join(hp_rows)\n",
    "\n",
    "# Run Info\n",
    "run_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?algoLabel ?start ?end ?metricLabel ?metricVal WHERE {{\n",
    "  OPTIONAL {{ :train_and_finetune_model prov:startedAtTime ?start ; prov:endedAtTime ?end . }}\n",
    "  OPTIONAL {{\n",
    "      ?run sc:isPartOf :train_and_finetune_model .\n",
    "      ?run mls:realizes ?algo .\n",
    "      ?algo rdfs:label ?algoLabel .\n",
    "  }}\n",
    "  OPTIONAL {{\n",
    "    ?run sc:isPartOf :train_and_finetune_model .\n",
    "    ?run mls:hasOutput ?eval .\n",
    "    ?eval a mls:ModelEvaluation ; mls:hasValue ?metricVal .\n",
    "    OPTIONAL {{ ?eval mls:specifiedBy ?m . ?m rdfs:label ?metricLabel . }}\n",
    "  }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_run = engine.query(run_query)\n",
    "row_run = res_run.iloc[0] if not res_run.empty else {} #type:ignore\n",
    "mod_algo  = latex_escape(clean_rdf(row_run.get(\"algoLabel\", \"\")))\n",
    "mod_start = latex_escape(fmt_iso(clean_rdf(row_run.get(\"start\"))))\n",
    "mod_end   = latex_escape(fmt_iso(clean_rdf(row_run.get(\"end\"))))\n",
    "mod_m_lbl = latex_escape(clean_rdf(row_run.get(\"metricLabel\", \"\")))\n",
    "raw_val = clean_rdf(row_run.get('metricVal', ''))\n",
    "mod_m_val = f\"{float(raw_val):.4f}\" if raw_val else \"\"\n",
    "\n",
    "print(\"Data extraction done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8fa1c",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "The following features were identified in the dataset:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Data Cleaning}}\n",
    "Describe your Data preparation steps here and include respective graph data.\n",
    "\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.4\\linewidth}}l}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Training Run}}\n",
    "A training run was executed with the following characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Start Time:}} {mod_start}\n",
    "    \\item \\textbf{{End Time:}} {mod_end}\n",
    "    \\item \\textbf{{Result:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c947b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed183a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BI2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
